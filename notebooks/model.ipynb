{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "INPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 1024 // 2\n",
    "LINEAR_INPUT_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.003 #0.0005\n",
    "L2_WEIGTH_DECAY = 0.0001\n",
    "EPOCHS = 10\n",
    "\n",
    "LABELS = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4,\n",
    "    6: 5,\n",
    "    7: 6,\n",
    "    8: 7,\n",
    "    9: 8,\n",
    "    10: 9,\n",
    "    11: 10,\n",
    "    13: 11,\n",
    "    15: 12,\n",
    "    17: 13,\n",
    "    19: 14,\n",
    "    20: 15,\n",
    "    22: 16,\n",
    "    23: 17,\n",
    "    25: 18,\n",
    "    28: 19,\n",
    "    29: 20,\n",
    "    30: 21,\n",
    "    31: 22,\n",
    "    32: 23,\n",
    "    33: 24,\n",
    "    34: 25,\n",
    "    35: 26,\n",
    "    36: 27,\n",
    "    37: 28,\n",
    "    38: 29,\n",
    "    39: 30,\n",
    "    40: 31,\n",
    "    41: 32,\n",
    "    42: 33,\n",
    "    43: 34,\n",
    "    44: 35,\n",
    "    45: 36,\n",
    "    46: 37,\n",
    "    47: 38,\n",
    "    48: 39,\n",
    "    49: 40,\n",
    "    50: 41,\n",
    "    51: 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMovementDataset(IterableDataset):\n",
    "    \n",
    "    NUMBER_OF_JOINTS = 25\n",
    "    NUMBER_OF_AXES = 3\n",
    "    \n",
    "    def __init__(self, root: str, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.data_files = list(sorted(os.listdir(self.root)))\n",
    "        self.file_frames: List[int] = []\n",
    "        \n",
    "        self.classes = LABELS\n",
    "        \n",
    "        self.loaded_data = dict()\n",
    "        for file_name in self.data_files:\n",
    "            with open(os.path.join(self.root, file_name), \"r\") as f:\n",
    "                self.loaded_data[file_name] = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    def _get_file_length(self, file_data: List[str]):\n",
    "        header = file_data[0].split()[-1].split(\"_\")\n",
    "        return int(header[-1])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, file_name in enumerate(self.data_files):\n",
    "            #action_file = os.path.join(self.root, file_name)\n",
    "            #with open(action_file, \"r\") as f:\n",
    "            #    data_str = f.read().rstrip('\\n').split('\\n')\n",
    "            data_str = self.loaded_data[file_name]\n",
    "            \n",
    "            sequence_length = self._get_file_length(data_str)\n",
    "            \n",
    "            all_frames = []\n",
    "            for frame in data_str[2:]:  # first two header lines in the file\n",
    "                all_frames.append(\n",
    "                    [triple.split(\", \") for triple in frame.split(\"; \")]\n",
    "                )\n",
    "            \n",
    "            all_frames = np.array(all_frames, dtype=np.float32)\n",
    "            '''\n",
    "            frame = np.array(\n",
    "                [\n",
    "                    triple.split(\", \") for triple in data_str[line_indx].split(\";\")\n",
    "                ],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            '''\n",
    "            assert all_frames.shape == (sequence_length, self.NUMBER_OF_JOINTS, self.NUMBER_OF_AXES)\n",
    "    \n",
    "            # get sequence label\n",
    "            target = [self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]] * sequence_length\n",
    "            target = np.array(target)\n",
    "        \n",
    "            if self.transforms:\n",
    "                all_frames = self.transforms(all_frames)\n",
    "\n",
    "            yield all_frames, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, linear_input_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(self.hidden_size * 2, linear_input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(linear_input_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward to LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # output format: (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.embedding(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.do(out)\n",
    "        out = self.classifier(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch dataloader\n",
    "class MovementsDataset(Dataset):\n",
    "    \n",
    "    NUMBER_OF_JOINTS = 25\n",
    "    NUMBER_OF_AXES = 3\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.data_files = list(sorted(os.listdir(self.root)))\n",
    "        self.file_frames: List[int] = []\n",
    "\n",
    "        self.classes = LABELS\n",
    "        \n",
    "        # Load number of frames for every file\n",
    "        for fn in self.data_files:\n",
    "            with open(os.path.join(self.root, fn)) as f:\n",
    "                header = f.readline().split()[-1].split(\"_\")\n",
    "                \n",
    "                self.file_frames.append(int(header[-1]))  # last element - number_of_frames\n",
    "        \n",
    "    def _get_file_index(self, frame_indx) -> Tuple[int, int]:\n",
    "        start_indx = frame_indx\n",
    "        for i, nof in enumerate(self.file_frames):\n",
    "            if start_indx < nof:\n",
    "                # print(f\"{start_indx} - {i}\")\n",
    "                return i, start_indx\n",
    "            else:\n",
    "                start_indx -= nof\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        file_indx, line_indx = self._get_file_index(indx)\n",
    "        action_file = os.path.join(self.root, self.data_files[file_indx])\n",
    "        \n",
    "        with open(action_file, \"r\") as f:\n",
    "            data_str = f.read().rstrip('\\n').split('\\n')\n",
    "        \n",
    "        line_indx += 2  # first two header lines in the file   \n",
    "        frame = np.array([triple.split(\", \") for triple in data_str[line_indx].split(\";\")], dtype=np.float32)\n",
    "        assert frame.shape == (self.NUMBER_OF_JOINTS, self.NUMBER_OF_AXES)\n",
    "        \n",
    "        target = self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]\n",
    "        \n",
    "        if self.transforms:\n",
    "            frame = self.transforms(frame)\n",
    "        \n",
    "        return frame, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IterableMovementDataset(\n",
    "    \"../data/cross-subject/train\",\n",
    "    transforms=transforms.ToTensor()\n",
    ")\n",
    "test_dataset = IterableMovementDataset(\n",
    "    \"../data/cross-subject/val\",\n",
    "    transforms=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset) #, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset) #, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "#  - Adam optimizer *\n",
    "#  - LR = 0.0005 *\n",
    "#  - batch = 1 *\n",
    "#  - L2 weight decay = 0.0001 *\n",
    "#  - dropout = 0.5 *\n",
    "#  - 200 epochs\n",
    "#  - Embedding - 64 *\n",
    "#  - Hidden-state - 1024 --> halved for Bi-LSTM *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "999/17311 -  Epoch [1/10], average_loss: 3.4436\n",
      "1998/17311 -  Epoch [1/10], average_loss: 3.4168\n",
      "2997/17311 -  Epoch [1/10], average_loss: 3.4205\n",
      "3996/17311 -  Epoch [1/10], average_loss: 3.3931\n",
      "4995/17311 -  Epoch [1/10], average_loss: 3.4031\n",
      "5994/17311 -  Epoch [1/10], average_loss: 3.4089\n",
      "6993/17311 -  Epoch [1/10], average_loss: 3.3753\n",
      "7992/17311 -  Epoch [1/10], average_loss: 3.4082\n",
      "8991/17311 -  Epoch [1/10], average_loss: 3.3676\n",
      "9990/17311 -  Epoch [1/10], average_loss: 3.3478\n",
      "10989/17311 -  Epoch [1/10], average_loss: 3.3945\n",
      "11988/17311 -  Epoch [1/10], average_loss: 3.3641\n",
      "12987/17311 -  Epoch [1/10], average_loss: 3.42\n",
      "13986/17311 -  Epoch [1/10], average_loss: 3.3764\n",
      "14985/17311 -  Epoch [1/10], average_loss: 3.4207\n",
      "15984/17311 -  Epoch [1/10], average_loss: 3.3365\n",
      "16983/17311 -  Epoch [1/10], average_loss: 3.3594\n",
      "Evaluation time: 152.66332149505615s.\n",
      "999/17311 -  Epoch [2/10], average_loss: 3.4192\n",
      "1998/17311 -  Epoch [2/10], average_loss: 3.4063\n",
      "2997/17311 -  Epoch [2/10], average_loss: 3.406\n",
      "3996/17311 -  Epoch [2/10], average_loss: 3.3876\n",
      "4995/17311 -  Epoch [2/10], average_loss: 3.3858\n",
      "5994/17311 -  Epoch [2/10], average_loss: 3.3993\n",
      "6993/17311 -  Epoch [2/10], average_loss: 3.3687\n",
      "7992/17311 -  Epoch [2/10], average_loss: 3.4003\n",
      "8991/17311 -  Epoch [2/10], average_loss: 3.3601\n",
      "9990/17311 -  Epoch [2/10], average_loss: 3.346\n",
      "10989/17311 -  Epoch [2/10], average_loss: 3.3893\n",
      "11988/17311 -  Epoch [2/10], average_loss: 3.3495\n",
      "12987/17311 -  Epoch [2/10], average_loss: 3.4087\n",
      "13986/17311 -  Epoch [2/10], average_loss: 3.3672\n",
      "14985/17311 -  Epoch [2/10], average_loss: 3.415\n",
      "15984/17311 -  Epoch [2/10], average_loss: 3.33\n",
      "16983/17311 -  Epoch [2/10], average_loss: 3.3588\n",
      "Evaluation time: 153.44956183433533s.\n",
      "999/17311 -  Epoch [3/10], average_loss: 3.4144\n",
      "1998/17311 -  Epoch [3/10], average_loss: 3.4006\n",
      "2997/17311 -  Epoch [3/10], average_loss: 3.3978\n",
      "3996/17311 -  Epoch [3/10], average_loss: 3.3822\n",
      "4995/17311 -  Epoch [3/10], average_loss: 3.3847\n",
      "5994/17311 -  Epoch [3/10], average_loss: 3.4\n",
      "6993/17311 -  Epoch [3/10], average_loss: 3.3571\n",
      "7992/17311 -  Epoch [3/10], average_loss: 3.3944\n",
      "8991/17311 -  Epoch [3/10], average_loss: 3.3534\n",
      "9990/17311 -  Epoch [3/10], average_loss: 3.3351\n",
      "10989/17311 -  Epoch [3/10], average_loss: 3.3798\n",
      "11988/17311 -  Epoch [3/10], average_loss: 3.349\n",
      "12987/17311 -  Epoch [3/10], average_loss: 3.404\n",
      "13986/17311 -  Epoch [3/10], average_loss: 3.3613\n",
      "14985/17311 -  Epoch [3/10], average_loss: 3.412\n",
      "15984/17311 -  Epoch [3/10], average_loss: 3.3271\n",
      "16983/17311 -  Epoch [3/10], average_loss: 3.3532\n",
      "Evaluation time: 151.63485980033875s.\n",
      "999/17311 -  Epoch [4/10], average_loss: 3.4176\n",
      "1998/17311 -  Epoch [4/10], average_loss: 3.396\n",
      "2997/17311 -  Epoch [4/10], average_loss: 3.3948\n",
      "3996/17311 -  Epoch [4/10], average_loss: 3.3732\n",
      "4995/17311 -  Epoch [4/10], average_loss: 3.3737\n",
      "5994/17311 -  Epoch [4/10], average_loss: 3.3956\n",
      "6993/17311 -  Epoch [4/10], average_loss: 3.3548\n",
      "7992/17311 -  Epoch [4/10], average_loss: 3.3962\n",
      "8991/17311 -  Epoch [4/10], average_loss: 3.3526\n",
      "9990/17311 -  Epoch [4/10], average_loss: 3.3391\n",
      "10989/17311 -  Epoch [4/10], average_loss: 3.3761\n",
      "11988/17311 -  Epoch [4/10], average_loss: 3.3412\n",
      "12987/17311 -  Epoch [4/10], average_loss: 3.3979\n",
      "13986/17311 -  Epoch [4/10], average_loss: 3.3578\n",
      "14985/17311 -  Epoch [4/10], average_loss: 3.409\n",
      "15984/17311 -  Epoch [4/10], average_loss: 3.3238\n",
      "16983/17311 -  Epoch [4/10], average_loss: 3.3517\n",
      "Evaluation time: 155.3442358970642s.\n",
      "999/17311 -  Epoch [5/10], average_loss: 3.4124\n",
      "1998/17311 -  Epoch [5/10], average_loss: 3.3934\n",
      "2997/17311 -  Epoch [5/10], average_loss: 3.3857\n",
      "3996/17311 -  Epoch [5/10], average_loss: 3.3636\n",
      "4995/17311 -  Epoch [5/10], average_loss: 3.3534\n",
      "5994/17311 -  Epoch [5/10], average_loss: 3.3852\n",
      "6993/17311 -  Epoch [5/10], average_loss: 3.3519\n",
      "7992/17311 -  Epoch [5/10], average_loss: 3.3848\n",
      "8991/17311 -  Epoch [5/10], average_loss: 3.3436\n",
      "9990/17311 -  Epoch [5/10], average_loss: 3.3257\n",
      "10989/17311 -  Epoch [5/10], average_loss: 3.3702\n",
      "11988/17311 -  Epoch [5/10], average_loss: 3.3391\n",
      "12987/17311 -  Epoch [5/10], average_loss: 3.3949\n",
      "13986/17311 -  Epoch [5/10], average_loss: 3.3539\n",
      "14985/17311 -  Epoch [5/10], average_loss: 3.411\n",
      "15984/17311 -  Epoch [5/10], average_loss: 3.3195\n",
      "16983/17311 -  Epoch [5/10], average_loss: 3.3482\n",
      "Evaluation time: 153.16743087768555s.\n",
      "999/17311 -  Epoch [6/10], average_loss: 3.4066\n",
      "1998/17311 -  Epoch [6/10], average_loss: 3.3882\n",
      "2997/17311 -  Epoch [6/10], average_loss: 3.3844\n",
      "3996/17311 -  Epoch [6/10], average_loss: 3.3632\n",
      "4995/17311 -  Epoch [6/10], average_loss: 3.3746\n",
      "5994/17311 -  Epoch [6/10], average_loss: 3.3874\n",
      "6993/17311 -  Epoch [6/10], average_loss: 3.3507\n",
      "7992/17311 -  Epoch [6/10], average_loss: 3.3898\n",
      "8991/17311 -  Epoch [6/10], average_loss: 3.3409\n",
      "9990/17311 -  Epoch [6/10], average_loss: 3.3286\n",
      "10989/17311 -  Epoch [6/10], average_loss: 3.3711\n",
      "11988/17311 -  Epoch [6/10], average_loss: 3.3367\n",
      "12987/17311 -  Epoch [6/10], average_loss: 3.393\n",
      "13986/17311 -  Epoch [6/10], average_loss: 3.3537\n",
      "14985/17311 -  Epoch [6/10], average_loss: 3.4171\n",
      "15984/17311 -  Epoch [6/10], average_loss: 3.3186\n",
      "16983/17311 -  Epoch [6/10], average_loss: 3.3476\n",
      "Evaluation time: 155.0627179145813s.\n",
      "999/17311 -  Epoch [7/10], average_loss: 3.4061\n",
      "1998/17311 -  Epoch [7/10], average_loss: 3.3861\n",
      "2997/17311 -  Epoch [7/10], average_loss: 3.382\n",
      "3996/17311 -  Epoch [7/10], average_loss: 3.3652\n",
      "4995/17311 -  Epoch [7/10], average_loss: 3.3766\n",
      "5994/17311 -  Epoch [7/10], average_loss: 3.3858\n",
      "6993/17311 -  Epoch [7/10], average_loss: 3.3471\n",
      "7992/17311 -  Epoch [7/10], average_loss: 3.3821\n",
      "8991/17311 -  Epoch [7/10], average_loss: 3.3414\n",
      "9990/17311 -  Epoch [7/10], average_loss: 3.3354\n",
      "10989/17311 -  Epoch [7/10], average_loss: 3.3679\n",
      "11988/17311 -  Epoch [7/10], average_loss: 3.3359\n",
      "12987/17311 -  Epoch [7/10], average_loss: 3.3978\n",
      "13986/17311 -  Epoch [7/10], average_loss: 3.3501\n",
      "14985/17311 -  Epoch [7/10], average_loss: 3.405\n",
      "15984/17311 -  Epoch [7/10], average_loss: 3.3153\n",
      "16983/17311 -  Epoch [7/10], average_loss: 3.3456\n",
      "Evaluation time: 152.12217211723328s.\n",
      "999/17311 -  Epoch [8/10], average_loss: 3.407\n",
      "1998/17311 -  Epoch [8/10], average_loss: 3.3842\n",
      "2997/17311 -  Epoch [8/10], average_loss: 3.3781\n",
      "3996/17311 -  Epoch [8/10], average_loss: 3.3618\n",
      "4995/17311 -  Epoch [8/10], average_loss: 3.3712\n",
      "5994/17311 -  Epoch [8/10], average_loss: 3.38\n",
      "6993/17311 -  Epoch [8/10], average_loss: 3.3451\n",
      "7992/17311 -  Epoch [8/10], average_loss: 3.3812\n",
      "8991/17311 -  Epoch [8/10], average_loss: 3.3459\n",
      "9990/17311 -  Epoch [8/10], average_loss: 3.3294\n",
      "10989/17311 -  Epoch [8/10], average_loss: 3.3679\n",
      "11988/17311 -  Epoch [8/10], average_loss: 3.3391\n",
      "12987/17311 -  Epoch [8/10], average_loss: 3.3938\n",
      "13986/17311 -  Epoch [8/10], average_loss: 3.352\n",
      "14985/17311 -  Epoch [8/10], average_loss: 3.4068\n",
      "15984/17311 -  Epoch [8/10], average_loss: 3.3202\n",
      "16983/17311 -  Epoch [8/10], average_loss: 3.3468\n",
      "Evaluation time: 153.6420121192932s.\n",
      "999/17311 -  Epoch [9/10], average_loss: 3.3983\n",
      "1998/17311 -  Epoch [9/10], average_loss: 3.3773\n",
      "2997/17311 -  Epoch [9/10], average_loss: 3.3838\n",
      "3996/17311 -  Epoch [9/10], average_loss: 3.3622\n",
      "4995/17311 -  Epoch [9/10], average_loss: 3.3635\n",
      "5994/17311 -  Epoch [9/10], average_loss: 3.3794\n",
      "6993/17311 -  Epoch [9/10], average_loss: 3.3439\n",
      "7992/17311 -  Epoch [9/10], average_loss: 3.3787\n",
      "8991/17311 -  Epoch [9/10], average_loss: 3.3429\n",
      "9990/17311 -  Epoch [9/10], average_loss: 3.3389\n",
      "10989/17311 -  Epoch [9/10], average_loss: 3.3616\n",
      "11988/17311 -  Epoch [9/10], average_loss: 3.3319\n",
      "12987/17311 -  Epoch [9/10], average_loss: 3.3904\n",
      "13986/17311 -  Epoch [9/10], average_loss: 3.3494\n",
      "14985/17311 -  Epoch [9/10], average_loss: 3.4075\n",
      "15984/17311 -  Epoch [9/10], average_loss: 3.3143\n",
      "16983/17311 -  Epoch [9/10], average_loss: 3.3446\n",
      "Evaluation time: 156.7541356086731s.\n",
      "999/17311 -  Epoch [10/10], average_loss: 3.401\n",
      "1998/17311 -  Epoch [10/10], average_loss: 3.3837\n",
      "2997/17311 -  Epoch [10/10], average_loss: 3.3763\n",
      "3996/17311 -  Epoch [10/10], average_loss: 3.3605\n",
      "4995/17311 -  Epoch [10/10], average_loss: 3.3645\n",
      "5994/17311 -  Epoch [10/10], average_loss: 3.3818\n",
      "6993/17311 -  Epoch [10/10], average_loss: 3.3457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7992/17311 -  Epoch [10/10], average_loss: 3.3726\n",
      "8991/17311 -  Epoch [10/10], average_loss: 3.3331\n",
      "9990/17311 -  Epoch [10/10], average_loss: 3.3293\n",
      "10989/17311 -  Epoch [10/10], average_loss: 3.3664\n",
      "11988/17311 -  Epoch [10/10], average_loss: 3.3328\n",
      "12987/17311 -  Epoch [10/10], average_loss: 3.3902\n",
      "13986/17311 -  Epoch [10/10], average_loss: 3.3463\n",
      "14985/17311 -  Epoch [10/10], average_loss: 3.4017\n",
      "15984/17311 -  Epoch [10/10], average_loss: 3.3142\n",
      "16983/17311 -  Epoch [10/10], average_loss: 3.3465\n",
      "Evaluation time: 154.52292799949646s.\n",
      "999/17311 -  Epoch [11/10], average_loss: 3.3996\n",
      "1998/17311 -  Epoch [11/10], average_loss: 3.3832\n",
      "2997/17311 -  Epoch [11/10], average_loss: 3.373\n",
      "3996/17311 -  Epoch [11/10], average_loss: 3.3598\n",
      "4995/17311 -  Epoch [11/10], average_loss: 3.359\n",
      "5994/17311 -  Epoch [11/10], average_loss: 3.3726\n",
      "6993/17311 -  Epoch [11/10], average_loss: 3.3431\n",
      "7992/17311 -  Epoch [11/10], average_loss: 3.3764\n",
      "8991/17311 -  Epoch [11/10], average_loss: 3.334\n",
      "9990/17311 -  Epoch [11/10], average_loss: 3.323\n",
      "10989/17311 -  Epoch [11/10], average_loss: 3.3604\n",
      "11988/17311 -  Epoch [11/10], average_loss: 3.3316\n",
      "12987/17311 -  Epoch [11/10], average_loss: 3.3868\n",
      "13986/17311 -  Epoch [11/10], average_loss: 3.3484\n",
      "14985/17311 -  Epoch [11/10], average_loss: 3.3994\n",
      "15984/17311 -  Epoch [11/10], average_loss: 3.3128\n",
      "16983/17311 -  Epoch [11/10], average_loss: 3.3431\n",
      "Evaluation time: 154.30422234535217s.\n",
      "999/17311 -  Epoch [12/10], average_loss: 3.3962\n",
      "1998/17311 -  Epoch [12/10], average_loss: 3.3846\n",
      "2997/17311 -  Epoch [12/10], average_loss: 3.3782\n",
      "3996/17311 -  Epoch [12/10], average_loss: 3.3574\n",
      "4995/17311 -  Epoch [12/10], average_loss: 3.3558\n",
      "5994/17311 -  Epoch [12/10], average_loss: 3.3786\n",
      "6993/17311 -  Epoch [12/10], average_loss: 3.3441\n",
      "7992/17311 -  Epoch [12/10], average_loss: 3.378\n",
      "8991/17311 -  Epoch [12/10], average_loss: 3.3317\n",
      "9990/17311 -  Epoch [12/10], average_loss: 3.3211\n",
      "10989/17311 -  Epoch [12/10], average_loss: 3.3618\n",
      "11988/17311 -  Epoch [12/10], average_loss: 3.3318\n",
      "12987/17311 -  Epoch [12/10], average_loss: 3.3899\n",
      "13986/17311 -  Epoch [12/10], average_loss: 3.3476\n",
      "14985/17311 -  Epoch [12/10], average_loss: 3.405\n",
      "15984/17311 -  Epoch [12/10], average_loss: 3.3135\n",
      "16983/17311 -  Epoch [12/10], average_loss: 3.342\n",
      "Evaluation time: 154.95015358924866s.\n",
      "999/17311 -  Epoch [13/10], average_loss: 3.3972\n",
      "1998/17311 -  Epoch [13/10], average_loss: 3.3799\n",
      "2997/17311 -  Epoch [13/10], average_loss: 3.3729\n",
      "3996/17311 -  Epoch [13/10], average_loss: 3.3535\n",
      "4995/17311 -  Epoch [13/10], average_loss: 3.3525\n",
      "5994/17311 -  Epoch [13/10], average_loss: 3.3763\n",
      "6993/17311 -  Epoch [13/10], average_loss: 3.3402\n",
      "7992/17311 -  Epoch [13/10], average_loss: 3.3734\n",
      "8991/17311 -  Epoch [13/10], average_loss: 3.3365\n",
      "9990/17311 -  Epoch [13/10], average_loss: 3.3237\n",
      "10989/17311 -  Epoch [13/10], average_loss: 3.3583\n",
      "11988/17311 -  Epoch [13/10], average_loss: 3.3295\n",
      "12987/17311 -  Epoch [13/10], average_loss: 3.3857\n",
      "13986/17311 -  Epoch [13/10], average_loss: 3.3487\n",
      "14985/17311 -  Epoch [13/10], average_loss: 3.4033\n",
      "15984/17311 -  Epoch [13/10], average_loss: 3.3137\n",
      "16983/17311 -  Epoch [13/10], average_loss: 3.3477\n",
      "Evaluation time: 154.98309350013733s.\n",
      "999/17311 -  Epoch [14/10], average_loss: 3.3945\n",
      "1998/17311 -  Epoch [14/10], average_loss: 3.3813\n",
      "2997/17311 -  Epoch [14/10], average_loss: 3.3796\n",
      "3996/17311 -  Epoch [14/10], average_loss: 3.3604\n",
      "4995/17311 -  Epoch [14/10], average_loss: 3.3683\n",
      "5994/17311 -  Epoch [14/10], average_loss: 3.3747\n",
      "6993/17311 -  Epoch [14/10], average_loss: 3.3421\n",
      "7992/17311 -  Epoch [14/10], average_loss: 3.3731\n",
      "8991/17311 -  Epoch [14/10], average_loss: 3.3376\n",
      "9990/17311 -  Epoch [14/10], average_loss: 3.3256\n",
      "10989/17311 -  Epoch [14/10], average_loss: 3.3601\n",
      "11988/17311 -  Epoch [14/10], average_loss: 3.3331\n",
      "12987/17311 -  Epoch [14/10], average_loss: 3.386\n",
      "13986/17311 -  Epoch [14/10], average_loss: 3.3435\n",
      "14985/17311 -  Epoch [14/10], average_loss: 3.4089\n",
      "15984/17311 -  Epoch [14/10], average_loss: 3.3121\n",
      "16983/17311 -  Epoch [14/10], average_loss: 3.344\n",
      "Evaluation time: 151.6964831352234s.\n",
      "999/17311 -  Epoch [15/10], average_loss: 3.3979\n",
      "1998/17311 -  Epoch [15/10], average_loss: 3.3799\n",
      "2997/17311 -  Epoch [15/10], average_loss: 3.3802\n",
      "3996/17311 -  Epoch [15/10], average_loss: 3.3624\n",
      "4995/17311 -  Epoch [15/10], average_loss: 3.3697\n",
      "5994/17311 -  Epoch [15/10], average_loss: 3.3795\n",
      "6993/17311 -  Epoch [15/10], average_loss: 3.3416\n",
      "7992/17311 -  Epoch [15/10], average_loss: 3.3761\n",
      "8991/17311 -  Epoch [15/10], average_loss: 3.3381\n",
      "9990/17311 -  Epoch [15/10], average_loss: 3.334\n",
      "10989/17311 -  Epoch [15/10], average_loss: 3.3586\n",
      "11988/17311 -  Epoch [15/10], average_loss: 3.33\n",
      "12987/17311 -  Epoch [15/10], average_loss: 3.3863\n",
      "13986/17311 -  Epoch [15/10], average_loss: 3.3452\n",
      "14985/17311 -  Epoch [15/10], average_loss: 3.4004\n",
      "15984/17311 -  Epoch [15/10], average_loss: 3.3103\n",
      "16983/17311 -  Epoch [15/10], average_loss: 3.3408\n",
      "Evaluation time: 153.9512379169464s.\n",
      "999/17311 -  Epoch [16/10], average_loss: 3.4014\n",
      "1998/17311 -  Epoch [16/10], average_loss: 3.3839\n",
      "2997/17311 -  Epoch [16/10], average_loss: 3.3759\n",
      "3996/17311 -  Epoch [16/10], average_loss: 3.3589\n",
      "4995/17311 -  Epoch [16/10], average_loss: 3.3592\n",
      "5994/17311 -  Epoch [16/10], average_loss: 3.3744\n",
      "6993/17311 -  Epoch [16/10], average_loss: 3.338\n",
      "7992/17311 -  Epoch [16/10], average_loss: 3.3737\n",
      "8991/17311 -  Epoch [16/10], average_loss: 3.3378\n",
      "9990/17311 -  Epoch [16/10], average_loss: 3.3255\n",
      "10989/17311 -  Epoch [16/10], average_loss: 3.361\n",
      "11988/17311 -  Epoch [16/10], average_loss: 3.326\n",
      "12987/17311 -  Epoch [16/10], average_loss: 3.3879\n",
      "13986/17311 -  Epoch [16/10], average_loss: 3.3449\n",
      "14985/17311 -  Epoch [16/10], average_loss: 3.4032\n",
      "15984/17311 -  Epoch [16/10], average_loss: 3.3123\n",
      "16983/17311 -  Epoch [16/10], average_loss: 3.341\n",
      "Evaluation time: 153.13283967971802s.\n",
      "999/17311 -  Epoch [17/10], average_loss: 3.3983\n",
      "1998/17311 -  Epoch [17/10], average_loss: 3.382\n",
      "2997/17311 -  Epoch [17/10], average_loss: 3.3808\n",
      "3996/17311 -  Epoch [17/10], average_loss: 3.3611\n",
      "4995/17311 -  Epoch [17/10], average_loss: 3.3691\n",
      "5994/17311 -  Epoch [17/10], average_loss: 3.3792\n",
      "6993/17311 -  Epoch [17/10], average_loss: 3.3428\n",
      "7992/17311 -  Epoch [17/10], average_loss: 3.374\n",
      "8991/17311 -  Epoch [17/10], average_loss: 3.3283\n",
      "9990/17311 -  Epoch [17/10], average_loss: 3.321\n",
      "10989/17311 -  Epoch [17/10], average_loss: 3.3613\n",
      "11988/17311 -  Epoch [17/10], average_loss: 3.3323\n",
      "12987/17311 -  Epoch [17/10], average_loss: 3.3934\n",
      "13986/17311 -  Epoch [17/10], average_loss: 3.3457\n",
      "14985/17311 -  Epoch [17/10], average_loss: 3.3954\n",
      "15984/17311 -  Epoch [17/10], average_loss: 3.3137\n",
      "16983/17311 -  Epoch [17/10], average_loss: 3.3403\n",
      "Evaluation time: 153.52685379981995s.\n",
      "999/17311 -  Epoch [18/10], average_loss: 3.3983\n",
      "1998/17311 -  Epoch [18/10], average_loss: 3.3833\n",
      "2997/17311 -  Epoch [18/10], average_loss: 3.3814\n",
      "3996/17311 -  Epoch [18/10], average_loss: 3.3644\n",
      "4995/17311 -  Epoch [18/10], average_loss: 3.3719\n",
      "5994/17311 -  Epoch [18/10], average_loss: 3.3771\n",
      "6993/17311 -  Epoch [18/10], average_loss: 3.3396\n",
      "7992/17311 -  Epoch [18/10], average_loss: 3.3748\n",
      "8991/17311 -  Epoch [18/10], average_loss: 3.3433\n",
      "9990/17311 -  Epoch [18/10], average_loss: 3.3363\n",
      "10989/17311 -  Epoch [18/10], average_loss: 3.3606\n",
      "11988/17311 -  Epoch [18/10], average_loss: 3.3279\n",
      "12987/17311 -  Epoch [18/10], average_loss: 3.3885\n",
      "13986/17311 -  Epoch [18/10], average_loss: 3.3456\n",
      "14985/17311 -  Epoch [18/10], average_loss: 3.3992\n",
      "15984/17311 -  Epoch [18/10], average_loss: 3.3132\n",
      "16983/17311 -  Epoch [18/10], average_loss: 3.3418\n",
      "Evaluation time: 151.72256755828857s.\n",
      "999/17311 -  Epoch [19/10], average_loss: 3.3972\n",
      "1998/17311 -  Epoch [19/10], average_loss: 3.3816\n",
      "2997/17311 -  Epoch [19/10], average_loss: 3.3811\n",
      "3996/17311 -  Epoch [19/10], average_loss: 3.3629\n",
      "4995/17311 -  Epoch [19/10], average_loss: 3.3706\n",
      "5994/17311 -  Epoch [19/10], average_loss: 3.3767\n",
      "6993/17311 -  Epoch [19/10], average_loss: 3.3382\n",
      "7992/17311 -  Epoch [19/10], average_loss: 3.3742\n",
      "8991/17311 -  Epoch [19/10], average_loss: 3.3379\n",
      "9990/17311 -  Epoch [19/10], average_loss: 3.3232\n",
      "10989/17311 -  Epoch [19/10], average_loss: 3.3577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11988/17311 -  Epoch [19/10], average_loss: 3.3324\n",
      "12987/17311 -  Epoch [19/10], average_loss: 3.3879\n",
      "13986/17311 -  Epoch [19/10], average_loss: 3.3443\n",
      "14985/17311 -  Epoch [19/10], average_loss: 3.3972\n",
      "15984/17311 -  Epoch [19/10], average_loss: 3.3109\n",
      "16983/17311 -  Epoch [19/10], average_loss: 3.3433\n",
      "Evaluation time: 152.6308238506317s.\n",
      "999/17311 -  Epoch [20/10], average_loss: 3.3959\n",
      "1998/17311 -  Epoch [20/10], average_loss: 3.3785\n",
      "2997/17311 -  Epoch [20/10], average_loss: 3.3836\n",
      "3996/17311 -  Epoch [20/10], average_loss: 3.3636\n",
      "4995/17311 -  Epoch [20/10], average_loss: 3.3691\n",
      "5994/17311 -  Epoch [20/10], average_loss: 3.3757\n",
      "6993/17311 -  Epoch [20/10], average_loss: 3.3388\n",
      "7992/17311 -  Epoch [20/10], average_loss: 3.3743\n",
      "8991/17311 -  Epoch [20/10], average_loss: 3.3388\n",
      "9990/17311 -  Epoch [20/10], average_loss: 3.3269\n",
      "10989/17311 -  Epoch [20/10], average_loss: 3.3587\n",
      "11988/17311 -  Epoch [20/10], average_loss: 3.3256\n",
      "12987/17311 -  Epoch [20/10], average_loss: 3.3881\n",
      "13986/17311 -  Epoch [20/10], average_loss: 3.3463\n",
      "14985/17311 -  Epoch [20/10], average_loss: 3.4037\n",
      "15984/17311 -  Epoch [20/10], average_loss: 3.3107\n",
      "16983/17311 -  Epoch [20/10], average_loss: 3.3399\n",
      "Evaluation time: 155.28273272514343s.\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "model = BiRNN(75, HIDDEN_SIZE, LINEAR_INPUT_SIZE, len(train_dataset.classes)).to(device)\n",
    "\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    print(\"Pretrained model not found\")\n",
    "\n",
    "PRINT_STEP = 999\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=L2_WEIGTH_DECAY\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(20):\n",
    "    s_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (sequence, labels) in enumerate(train_loader, 1):\n",
    "        #print(sequence.shape)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        sequence = sequence.reshape(-1, 1, 25 * 3).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "        \n",
    "        #print(sequence.shape)\n",
    "        #print(labels.shape)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(sequence)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % PRINT_STEP == 0:\n",
    "            print(f\"{i}/{len(train_loader)} -  Epoch [{epoch + 1}/{EPOCHS}], average_loss: {round(total_loss / PRINT_STEP, 4)}\")\n",
    "            total_loss = 0.0\n",
    "    \n",
    "    print(f\"Evaluation time: {time.time() - s_time}s.\")\n",
    "    #break\n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: [0/2509]\n",
      "Processed: [49/2509]\n",
      "Processed: [98/2509]\n",
      "Processed: [147/2509]\n",
      "Processed: [196/2509]\n",
      "Processed: [245/2509]\n",
      "Processed: [294/2509]\n",
      "Processed: [343/2509]\n",
      "Processed: [392/2509]\n",
      "Processed: [441/2509]\n",
      "Processed: [490/2509]\n",
      "Processed: [539/2509]\n",
      "Processed: [588/2509]\n",
      "Processed: [637/2509]\n",
      "Processed: [686/2509]\n",
      "Processed: [735/2509]\n",
      "Processed: [784/2509]\n",
      "Processed: [833/2509]\n",
      "Processed: [882/2509]\n",
      "Processed: [931/2509]\n",
      "Processed: [980/2509]\n",
      "Processed: [1029/2509]\n",
      "Processed: [1078/2509]\n",
      "Processed: [1127/2509]\n",
      "Processed: [1176/2509]\n",
      "Processed: [1225/2509]\n",
      "Processed: [1274/2509]\n",
      "Processed: [1323/2509]\n",
      "Processed: [1372/2509]\n",
      "Processed: [1421/2509]\n",
      "Processed: [1470/2509]\n",
      "Processed: [1519/2509]\n",
      "Processed: [1568/2509]\n",
      "Processed: [1617/2509]\n",
      "Processed: [1666/2509]\n",
      "Processed: [1715/2509]\n",
      "Processed: [1764/2509]\n",
      "Processed: [1813/2509]\n",
      "Processed: [1862/2509]\n",
      "Processed: [1911/2509]\n",
      "Processed: [1960/2509]\n",
      "Processed: [2009/2509]\n",
      "Processed: [2058/2509]\n",
      "Processed: [2107/2509]\n",
      "Processed: [2156/2509]\n",
      "Processed: [2205/2509]\n",
      "Processed: [2254/2509]\n",
      "Processed: [2303/2509]\n",
      "Processed: [2352/2509]\n",
      "Processed: [2401/2509]\n",
      "Processed: [2450/2509]\n",
      "Processed: [2499/2509]\n",
      "\n",
      "---------------0.1---------------\n",
      "Test Precision: 5.4376%\n",
      "Test Recall: 74.4751%\n",
      "Test f1_score: 0.1014\n",
      "\n",
      "---------------0.2---------------\n",
      "Test Precision: 5.8367%\n",
      "Test Recall: 71.1757%\n",
      "Test f1_score: 0.1079\n",
      "\n",
      "---------------0.3---------------\n",
      "Test Precision: 6.1035%\n",
      "Test Recall: 68.8708%\n",
      "Test f1_score: 0.1121\n",
      "\n",
      "---------------0.4---------------\n",
      "Test Precision: 6.3309%\n",
      "Test Recall: 67.0058%\n",
      "Test f1_score: 0.1157\n",
      "\n",
      "---------------0.5---------------\n",
      "Test Precision: 6.5401%\n",
      "Test Recall: 65.2694%\n",
      "Test f1_score: 0.1189\n",
      "\n",
      "---------------0.6---------------\n",
      "Test Precision: 6.7513%\n",
      "Test Recall: 63.5043%\n",
      "Test f1_score: 0.1221\n",
      "\n",
      "---------------0.7---------------\n",
      "Test Precision: 6.9892%\n",
      "Test Recall: 61.594%\n",
      "Test f1_score: 0.1255\n",
      "\n",
      "---------------0.8---------------\n",
      "Test Precision: 7.2804%\n",
      "Test Recall: 59.2102%\n",
      "Test f1_score: 0.1297\n",
      "\n",
      "---------------0.9---------------\n",
      "Test Precision: 7.7191%\n",
      "Test Recall: 55.4819%\n",
      "Test f1_score: 0.1355\n",
      "\n",
      "---------------1.0---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "with torch.no_grad():\n",
    "    steps = 10\n",
    "    def_dict = {\n",
    "        \"correct\": 0,\n",
    "        \"above\": 0,\n",
    "    }\n",
    "    thresholds = [(round((1 / steps) * (i + 1), 4), def_dict.copy()) for i in range(steps)]\n",
    "    total = 0\n",
    "\n",
    "    for i, (sequence, labels) in enumerate(test_loader):\n",
    "        sequence = sequence.reshape(-1, 1, 25 * 3).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "        \n",
    "        outputs = model(sequence)\n",
    "\n",
    "        for val_th, res_dict in thresholds:\n",
    "            res_dict[\"above\"] += (outputs.data > val_th).sum().item()\n",
    "            \n",
    "            # Correctly predicted\n",
    "            for record_i, label_i in zip(*torch.where(outputs.data > val_th)):\n",
    "                if label_i == labels[record_i]:\n",
    "                    res_dict[\"correct\"] += 1\n",
    "        \n",
    "        total += labels.size(0)\n",
    "\n",
    "        if i % 49 == 0:\n",
    "            print(f\"Processed: [{i}/{len(test_dataset)}]\")\n",
    "\n",
    "    ## Calculate result measurements for every threshold\n",
    "    for th, values in thresholds:\n",
    "        recall = values[\"correct\"] / total\n",
    "    \n",
    "        if values[\"above\"] > 0:\n",
    "            precision = values[\"correct\"] / values[\"above\"]\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            precision = 0\n",
    "            f1_score = 0\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 15 + f\"{th}\" + \"-\" * 15)\n",
    "        print(f\"Test Precision: {round(100 * precision, 4)}%\")\n",
    "        print(f\"Test Recall: {round(100 * recall, 4)}%\")\n",
    "        print(f\"Test f1_score: {round(f1_score, 4)}\")\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Evalution metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "#  - Precision: the ratio of correctly annotated frames and all the model-annotated frames on test sequences\n",
    "#  - spravne anotovane (correctly annotates)\n",
    "#  - zoberiem threshold pre kazdy output v kazdom snimku --> pocet tried do ktorych sa klasifikoval snimok (all model-annotated frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PKU-MMD measure\n",
    "# - evaluacia prebieha na celom datasette, THRESHOLD-0 je hranica kedy rozhodujem\n",
    "# - mam multi-class clasifikaciu, kedze beriem cely dataset, pozitivne/negativne do confusion-matrix\n",
    "#   a z toho vypocutam Recall & Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otazky\n",
    "#  - co je threshold - 0 ??\n",
    "#  - kedze klasifikujem tak beriem vysledok s najvacsou hodnotou, neklasifikujem na positivne/negativne ale\n",
    "#    na presnu kategoriu, takze ako mam zakomponovat threshold do toho?\n",
    "#  - Ako mam vyuzit Average Precision Protocol, chapem ako funguje IOU pri detekcii objektov, co toto je podobny\n",
    "#    princip, akurat si niesom isty ci moj vypocet je spravny kedze I u I* je vzdy v podsatate dlzka sekvencie\n",
    "#    nikdy to nebude nejake vacsie cislo (oproti IOU kde boxy mozu vytvorit roznu hodnotu zjednotenia), pretoze\n",
    "#    v tom pripade je ten vypocet v podsatate Accuracy, Pocet spravnych klasifokovanych framov / celkovy pocet?\n",
    "#  - taktiez k comu mam potom vyuzit ten threshold 0 pri Precision a Recall ?? Kedze vypocet je z confusion\n",
    "#    matrix, kde su kategorie jasne zadefinovane, kedze beriem vzdy maximum pre vsetky kategorie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
