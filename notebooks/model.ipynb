{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "INPUT_SIZE = 25 * 3\n",
    "HIDDEN_SIZE = 1024 // 2\n",
    "EMBEDDING_INPUT_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.003 #0.0005\n",
    "L2_WEIGTH_DECAY = 0.0001\n",
    "EPOCHS = 10\n",
    "\n",
    "LABELS = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4,\n",
    "    6: 5,\n",
    "    7: 6,\n",
    "    8: 7,\n",
    "    9: 8,\n",
    "    10: 9,\n",
    "    11: 10,\n",
    "    13: 11,\n",
    "    15: 12,\n",
    "    17: 13,\n",
    "    19: 14,\n",
    "    20: 15,\n",
    "    22: 16,\n",
    "    23: 17,\n",
    "    25: 18,\n",
    "    28: 19,\n",
    "    29: 20,\n",
    "    30: 21,\n",
    "    31: 22,\n",
    "    32: 23,\n",
    "    33: 24,\n",
    "    34: 25,\n",
    "    35: 26,\n",
    "    36: 27,\n",
    "    37: 28,\n",
    "    38: 29,\n",
    "    39: 30,\n",
    "    40: 31,\n",
    "    41: 32,\n",
    "    42: 33,\n",
    "    43: 34,\n",
    "    44: 35,\n",
    "    45: 36,\n",
    "    46: 37,\n",
    "    47: 38,\n",
    "    48: 39,\n",
    "    49: 40,\n",
    "    50: 41,\n",
    "    51: 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMovementDataset(IterableDataset):\n",
    "    \n",
    "    NUMBER_OF_JOINTS = 25\n",
    "    NUMBER_OF_AXES = 3\n",
    "    \n",
    "    def __init__(self, root: str, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.data_files = list(sorted(os.listdir(self.root)))\n",
    "        self.file_frames: List[int] = []\n",
    "        \n",
    "        self.classes = LABELS\n",
    "        \n",
    "        self.loaded_data = dict()\n",
    "        for file_name in self.data_files:\n",
    "            with open(os.path.join(self.root, file_name), \"r\") as f:\n",
    "                self.loaded_data[file_name] = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    def _get_file_length(self, file_data: List[str]):\n",
    "        header = file_data[0].split()[-1].split(\"_\")\n",
    "        return int(header[-1])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, file_name in enumerate(self.data_files):\n",
    "            #action_file = os.path.join(self.root, file_name)\n",
    "            #with open(action_file, \"r\") as f:\n",
    "            #    data_str = f.read().rstrip('\\n').split('\\n')\n",
    "            data_str = self.loaded_data[file_name]\n",
    "            \n",
    "            sequence_length = self._get_file_length(data_str)\n",
    "            \n",
    "            all_frames = []\n",
    "            for frame in data_str[2:]:  # first two header lines in the file\n",
    "                all_frames.append(\n",
    "                    [triple.split(\", \") for triple in frame.split(\"; \")]\n",
    "                )\n",
    "            \n",
    "            all_frames = np.array(all_frames, dtype=np.float32)\n",
    "            '''\n",
    "            frame = np.array(\n",
    "                [\n",
    "                    triple.split(\", \") for triple in data_str[line_indx].split(\";\")\n",
    "                ],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            '''\n",
    "            assert all_frames.shape == (sequence_length, self.NUMBER_OF_JOINTS, self.NUMBER_OF_AXES)\n",
    "    \n",
    "            # get sequence label\n",
    "            target = [self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]] * sequence_length\n",
    "            target = np.array(target)\n",
    "        \n",
    "            if self.transforms:\n",
    "                all_frames = self.transforms(all_frames)\n",
    "\n",
    "            yield all_frames, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, linear_input_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(self.hidden_size * 2, linear_input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(linear_input_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward to LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # output format: (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.embedding(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.do(out)\n",
    "        out = self.classifier(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewBiRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, lstm_hidden_size: int, embedding_output_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = lstm_hidden_size\n",
    "        self.num_layers = 2  # bi-LSTM\n",
    "        \n",
    "        # Embedding part, from 75 -> 64 size\n",
    "        self.embedding = nn.Linear(input_size, embedding_output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_output_size, lstm_hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.hidden_size * self.num_layers, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Embedding\n",
    "        out = self.embedding(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "        \n",
    "        out = self.do(out[:, -1, :])\n",
    "        out = self.classifier(out)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch dataloader\n",
    "class MovementsDataset(Dataset):\n",
    "    \n",
    "    NUMBER_OF_JOINTS = 25\n",
    "    NUMBER_OF_AXES = 3\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.data_files = list(sorted(os.listdir(self.root)))\n",
    "        self.file_frames: List[int] = []\n",
    "\n",
    "        self.classes = LABELS\n",
    "        \n",
    "        # Load number of frames for every file\n",
    "        for fn in self.data_files:\n",
    "            with open(os.path.join(self.root, fn)) as f:\n",
    "                header = f.readline().split()[-1].split(\"_\")\n",
    "                \n",
    "                self.file_frames.append(int(header[-1]))  # last element - number_of_frames\n",
    "        \n",
    "    def _get_file_index(self, frame_indx) -> Tuple[int, int]:\n",
    "        start_indx = frame_indx\n",
    "        for i, nof in enumerate(self.file_frames):\n",
    "            if start_indx < nof:\n",
    "                # print(f\"{start_indx} - {i}\")\n",
    "                return i, start_indx\n",
    "            else:\n",
    "                start_indx -= nof\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        file_indx, line_indx = self._get_file_index(indx)\n",
    "        action_file = os.path.join(self.root, self.data_files[file_indx])\n",
    "        \n",
    "        with open(action_file, \"r\") as f:\n",
    "            data_str = f.read().rstrip('\\n').split('\\n')\n",
    "        \n",
    "        line_indx += 2  # first two header lines in the file   \n",
    "        frame = np.array([triple.split(\", \") for triple in data_str[line_indx].split(\";\")], dtype=np.float32)\n",
    "        assert frame.shape == (self.NUMBER_OF_JOINTS, self.NUMBER_OF_AXES)\n",
    "        \n",
    "        target = self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]\n",
    "        \n",
    "        if self.transforms:\n",
    "            frame = self.transforms(frame)\n",
    "        \n",
    "        return frame, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IterableMovementDataset(\n",
    "    \"../data/cross-subject/train\",\n",
    "    transforms=transforms.ToTensor()\n",
    ")\n",
    "test_dataset = IterableMovementDataset(\n",
    "    \"../data/cross-subject/val\",\n",
    "    transforms=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset) #, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset) #, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "#  - Adam optimizer *\n",
    "#  - LR = 0.0005 *\n",
    "#  - batch = 1 *\n",
    "#  - L2 weight decay = 0.0001 *\n",
    "#  - dropout = 0.5 *\n",
    "#  - 200 epochs\n",
    "#  - Embedding - 64 *\n",
    "#  - Hidden-state - 1024 --> halved for Bi-LSTM *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "999/17311 -  Epoch [1/10], average_loss: 3.3571\n",
      "1998/17311 -  Epoch [1/10], average_loss: 3.3381\n",
      "2997/17311 -  Epoch [1/10], average_loss: 3.3388\n",
      "3996/17311 -  Epoch [1/10], average_loss: 3.3192\n",
      "4995/17311 -  Epoch [1/10], average_loss: 3.3314\n",
      "5994/17311 -  Epoch [1/10], average_loss: 3.3417\n",
      "6993/17311 -  Epoch [1/10], average_loss: 3.3026\n",
      "7992/17311 -  Epoch [1/10], average_loss: 3.3415\n",
      "8991/17311 -  Epoch [1/10], average_loss: 3.2882\n",
      "9990/17311 -  Epoch [1/10], average_loss: 3.3093\n",
      "10989/17311 -  Epoch [1/10], average_loss: 3.3133\n",
      "11988/17311 -  Epoch [1/10], average_loss: 3.2932\n",
      "12987/17311 -  Epoch [1/10], average_loss: 3.353\n",
      "13986/17311 -  Epoch [1/10], average_loss: 3.2959\n",
      "14985/17311 -  Epoch [1/10], average_loss: 3.3535\n",
      "15984/17311 -  Epoch [1/10], average_loss: 3.2695\n",
      "16983/17311 -  Epoch [1/10], average_loss: 3.2953\n",
      "Evaluation time: 157.76017332077026s.\n",
      "999/17311 -  Epoch [2/10], average_loss: 3.3542\n",
      "1998/17311 -  Epoch [2/10], average_loss: 3.341\n",
      "2997/17311 -  Epoch [2/10], average_loss: 3.3343\n",
      "3996/17311 -  Epoch [2/10], average_loss: 3.3201\n",
      "4995/17311 -  Epoch [2/10], average_loss: 3.3284\n",
      "5994/17311 -  Epoch [2/10], average_loss: 3.3384\n",
      "6993/17311 -  Epoch [2/10], average_loss: 3.3009\n",
      "7992/17311 -  Epoch [2/10], average_loss: 3.3393\n",
      "8991/17311 -  Epoch [2/10], average_loss: 3.2864\n",
      "9990/17311 -  Epoch [2/10], average_loss: 3.31\n",
      "10989/17311 -  Epoch [2/10], average_loss: 3.3155\n",
      "11988/17311 -  Epoch [2/10], average_loss: 3.2874\n",
      "12987/17311 -  Epoch [2/10], average_loss: 3.3555\n",
      "13986/17311 -  Epoch [2/10], average_loss: 3.2944\n",
      "14985/17311 -  Epoch [2/10], average_loss: 3.3522\n",
      "15984/17311 -  Epoch [2/10], average_loss: 3.2678\n",
      "16983/17311 -  Epoch [2/10], average_loss: 3.2916\n",
      "Evaluation time: 152.98045182228088s.\n",
      "999/17311 -  Epoch [3/10], average_loss: 3.3539\n",
      "1998/17311 -  Epoch [3/10], average_loss: 3.3402\n",
      "2997/17311 -  Epoch [3/10], average_loss: 3.3409\n",
      "3996/17311 -  Epoch [3/10], average_loss: 3.3183\n",
      "4995/17311 -  Epoch [3/10], average_loss: 3.328\n",
      "5994/17311 -  Epoch [3/10], average_loss: 3.3377\n",
      "6993/17311 -  Epoch [3/10], average_loss: 3.3037\n",
      "7992/17311 -  Epoch [3/10], average_loss: 3.3408\n",
      "8991/17311 -  Epoch [3/10], average_loss: 3.2889\n",
      "9990/17311 -  Epoch [3/10], average_loss: 3.3098\n",
      "10989/17311 -  Epoch [3/10], average_loss: 3.3102\n",
      "11988/17311 -  Epoch [3/10], average_loss: 3.2918\n",
      "12987/17311 -  Epoch [3/10], average_loss: 3.3504\n",
      "13986/17311 -  Epoch [3/10], average_loss: 3.2924\n",
      "14985/17311 -  Epoch [3/10], average_loss: 3.3507\n",
      "15984/17311 -  Epoch [3/10], average_loss: 3.2672\n",
      "16983/17311 -  Epoch [3/10], average_loss: 3.2922\n",
      "Evaluation time: 155.13937759399414s.\n",
      "999/17311 -  Epoch [4/10], average_loss: 3.3558\n",
      "1998/17311 -  Epoch [4/10], average_loss: 3.3394\n",
      "2997/17311 -  Epoch [4/10], average_loss: 3.3375\n",
      "3996/17311 -  Epoch [4/10], average_loss: 3.3197\n",
      "4995/17311 -  Epoch [4/10], average_loss: 3.3323\n",
      "5994/17311 -  Epoch [4/10], average_loss: 3.3417\n",
      "6993/17311 -  Epoch [4/10], average_loss: 3.3001\n",
      "7992/17311 -  Epoch [4/10], average_loss: 3.3381\n",
      "8991/17311 -  Epoch [4/10], average_loss: 3.2869\n",
      "9990/17311 -  Epoch [4/10], average_loss: 3.3087\n",
      "10989/17311 -  Epoch [4/10], average_loss: 3.3099\n",
      "11988/17311 -  Epoch [4/10], average_loss: 3.2923\n",
      "12987/17311 -  Epoch [4/10], average_loss: 3.3521\n",
      "13986/17311 -  Epoch [4/10], average_loss: 3.2958\n",
      "14985/17311 -  Epoch [4/10], average_loss: 3.3512\n",
      "15984/17311 -  Epoch [4/10], average_loss: 3.2695\n",
      "16983/17311 -  Epoch [4/10], average_loss: 3.2973\n",
      "Evaluation time: 155.0457022190094s.\n",
      "999/17311 -  Epoch [5/10], average_loss: 3.3524\n",
      "1998/17311 -  Epoch [5/10], average_loss: 3.3403\n",
      "2997/17311 -  Epoch [5/10], average_loss: 3.3352\n",
      "3996/17311 -  Epoch [5/10], average_loss: 3.3201\n",
      "4995/17311 -  Epoch [5/10], average_loss: 3.3284\n",
      "5994/17311 -  Epoch [5/10], average_loss: 3.3441\n",
      "6993/17311 -  Epoch [5/10], average_loss: 3.2975\n",
      "7992/17311 -  Epoch [5/10], average_loss: 3.3363\n",
      "8991/17311 -  Epoch [5/10], average_loss: 3.289\n",
      "9990/17311 -  Epoch [5/10], average_loss: 3.3099\n",
      "10989/17311 -  Epoch [5/10], average_loss: 3.3083\n",
      "11988/17311 -  Epoch [5/10], average_loss: 3.2923\n",
      "12987/17311 -  Epoch [5/10], average_loss: 3.3528\n",
      "13986/17311 -  Epoch [5/10], average_loss: 3.295\n",
      "14985/17311 -  Epoch [5/10], average_loss: 3.352\n",
      "15984/17311 -  Epoch [5/10], average_loss: 3.266\n",
      "16983/17311 -  Epoch [5/10], average_loss: 3.2938\n",
      "Evaluation time: 154.22589707374573s.\n",
      "999/17311 -  Epoch [6/10], average_loss: 3.3535\n",
      "1998/17311 -  Epoch [6/10], average_loss: 3.3404\n",
      "2997/17311 -  Epoch [6/10], average_loss: 3.3374\n",
      "3996/17311 -  Epoch [6/10], average_loss: 3.3219\n",
      "4995/17311 -  Epoch [6/10], average_loss: 3.3266\n",
      "5994/17311 -  Epoch [6/10], average_loss: 3.3421\n",
      "6993/17311 -  Epoch [6/10], average_loss: 3.301\n",
      "7992/17311 -  Epoch [6/10], average_loss: 3.3381\n",
      "8991/17311 -  Epoch [6/10], average_loss: 3.2875\n",
      "9990/17311 -  Epoch [6/10], average_loss: 3.3076\n",
      "10989/17311 -  Epoch [6/10], average_loss: 3.3099\n",
      "11988/17311 -  Epoch [6/10], average_loss: 3.2908\n",
      "12987/17311 -  Epoch [6/10], average_loss: 3.3549\n",
      "13986/17311 -  Epoch [6/10], average_loss: 3.2946\n",
      "14985/17311 -  Epoch [6/10], average_loss: 3.3515\n",
      "15984/17311 -  Epoch [6/10], average_loss: 3.2667\n",
      "16983/17311 -  Epoch [6/10], average_loss: 3.2911\n",
      "Evaluation time: 154.14634084701538s.\n",
      "999/17311 -  Epoch [7/10], average_loss: 3.3546\n",
      "1998/17311 -  Epoch [7/10], average_loss: 3.3437\n",
      "2997/17311 -  Epoch [7/10], average_loss: 3.3379\n",
      "3996/17311 -  Epoch [7/10], average_loss: 3.3224\n",
      "4995/17311 -  Epoch [7/10], average_loss: 3.328\n",
      "5994/17311 -  Epoch [7/10], average_loss: 3.3408\n",
      "6993/17311 -  Epoch [7/10], average_loss: 3.302\n",
      "7992/17311 -  Epoch [7/10], average_loss: 3.3384\n",
      "8991/17311 -  Epoch [7/10], average_loss: 3.2861\n",
      "9990/17311 -  Epoch [7/10], average_loss: 3.3121\n",
      "10989/17311 -  Epoch [7/10], average_loss: 3.3094\n",
      "11988/17311 -  Epoch [7/10], average_loss: 3.2888\n",
      "12987/17311 -  Epoch [7/10], average_loss: 3.3519\n",
      "13986/17311 -  Epoch [7/10], average_loss: 3.2945\n",
      "14985/17311 -  Epoch [7/10], average_loss: 3.3518\n",
      "15984/17311 -  Epoch [7/10], average_loss: 3.2659\n",
      "16983/17311 -  Epoch [7/10], average_loss: 3.2907\n",
      "Evaluation time: 153.1297309398651s.\n",
      "999/17311 -  Epoch [8/10], average_loss: 3.3503\n",
      "1998/17311 -  Epoch [8/10], average_loss: 3.3365\n",
      "2997/17311 -  Epoch [8/10], average_loss: 3.3339\n",
      "3996/17311 -  Epoch [8/10], average_loss: 3.3197\n",
      "4995/17311 -  Epoch [8/10], average_loss: 3.3285\n",
      "5994/17311 -  Epoch [8/10], average_loss: 3.3395\n",
      "6993/17311 -  Epoch [8/10], average_loss: 3.3015\n",
      "7992/17311 -  Epoch [8/10], average_loss: 3.3386\n",
      "8991/17311 -  Epoch [8/10], average_loss: 3.2845\n",
      "9990/17311 -  Epoch [8/10], average_loss: 3.3083\n",
      "10989/17311 -  Epoch [8/10], average_loss: 3.3098\n",
      "11988/17311 -  Epoch [8/10], average_loss: 3.2908\n",
      "12987/17311 -  Epoch [8/10], average_loss: 3.3511\n",
      "13986/17311 -  Epoch [8/10], average_loss: 3.2959\n",
      "14985/17311 -  Epoch [8/10], average_loss: 3.3541\n",
      "15984/17311 -  Epoch [8/10], average_loss: 3.2675\n",
      "16983/17311 -  Epoch [8/10], average_loss: 3.2931\n",
      "Evaluation time: 152.7480137348175s.\n",
      "999/17311 -  Epoch [9/10], average_loss: 3.356\n",
      "1998/17311 -  Epoch [9/10], average_loss: 3.3385\n",
      "2997/17311 -  Epoch [9/10], average_loss: 3.3352\n",
      "3996/17311 -  Epoch [9/10], average_loss: 3.3228\n",
      "4995/17311 -  Epoch [9/10], average_loss: 3.3327\n",
      "5994/17311 -  Epoch [9/10], average_loss: 3.3412\n",
      "6993/17311 -  Epoch [9/10], average_loss: 3.3026\n",
      "7992/17311 -  Epoch [9/10], average_loss: 3.339\n",
      "8991/17311 -  Epoch [9/10], average_loss: 3.2847\n",
      "9990/17311 -  Epoch [9/10], average_loss: 3.3095\n",
      "10989/17311 -  Epoch [9/10], average_loss: 3.3097\n",
      "11988/17311 -  Epoch [9/10], average_loss: 3.2864\n",
      "12987/17311 -  Epoch [9/10], average_loss: 3.3523\n",
      "13986/17311 -  Epoch [9/10], average_loss: 3.2955\n",
      "14985/17311 -  Epoch [9/10], average_loss: 3.3534\n",
      "15984/17311 -  Epoch [9/10], average_loss: 3.2681\n",
      "16983/17311 -  Epoch [9/10], average_loss: 3.2959\n",
      "Evaluation time: 157.08306527137756s.\n",
      "999/17311 -  Epoch [10/10], average_loss: 3.3529\n",
      "1998/17311 -  Epoch [10/10], average_loss: 3.338\n",
      "2997/17311 -  Epoch [10/10], average_loss: 3.3348\n",
      "3996/17311 -  Epoch [10/10], average_loss: 3.316\n",
      "4995/17311 -  Epoch [10/10], average_loss: 3.323\n",
      "5994/17311 -  Epoch [10/10], average_loss: 3.3362\n",
      "6993/17311 -  Epoch [10/10], average_loss: 3.3022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7992/17311 -  Epoch [10/10], average_loss: 3.3359\n",
      "8991/17311 -  Epoch [10/10], average_loss: 3.2865\n",
      "9990/17311 -  Epoch [10/10], average_loss: 3.3125\n",
      "10989/17311 -  Epoch [10/10], average_loss: 3.3075\n",
      "11988/17311 -  Epoch [10/10], average_loss: 3.2893\n",
      "12987/17311 -  Epoch [10/10], average_loss: 3.3503\n",
      "13986/17311 -  Epoch [10/10], average_loss: 3.2936\n",
      "14985/17311 -  Epoch [10/10], average_loss: 3.3505\n",
      "15984/17311 -  Epoch [10/10], average_loss: 3.266\n",
      "16983/17311 -  Epoch [10/10], average_loss: 3.2943\n",
      "Evaluation time: 153.55688571929932s.\n",
      "999/17311 -  Epoch [11/10], average_loss: 3.3507\n",
      "1998/17311 -  Epoch [11/10], average_loss: 3.3407\n",
      "2997/17311 -  Epoch [11/10], average_loss: 3.3359\n",
      "3996/17311 -  Epoch [11/10], average_loss: 3.3189\n",
      "4995/17311 -  Epoch [11/10], average_loss: 3.3271\n",
      "5994/17311 -  Epoch [11/10], average_loss: 3.3413\n",
      "6993/17311 -  Epoch [11/10], average_loss: 3.3003\n",
      "7992/17311 -  Epoch [11/10], average_loss: 3.3382\n",
      "8991/17311 -  Epoch [11/10], average_loss: 3.2868\n",
      "9990/17311 -  Epoch [11/10], average_loss: 3.311\n",
      "10989/17311 -  Epoch [11/10], average_loss: 3.306\n",
      "11988/17311 -  Epoch [11/10], average_loss: 3.2894\n",
      "12987/17311 -  Epoch [11/10], average_loss: 3.3509\n",
      "13986/17311 -  Epoch [11/10], average_loss: 3.2971\n",
      "14985/17311 -  Epoch [11/10], average_loss: 3.3505\n",
      "15984/17311 -  Epoch [11/10], average_loss: 3.2661\n",
      "16983/17311 -  Epoch [11/10], average_loss: 3.2949\n",
      "Evaluation time: 156.66005182266235s.\n",
      "999/17311 -  Epoch [12/10], average_loss: 3.3531\n",
      "1998/17311 -  Epoch [12/10], average_loss: 3.3369\n",
      "2997/17311 -  Epoch [12/10], average_loss: 3.3361\n",
      "3996/17311 -  Epoch [12/10], average_loss: 3.3151\n",
      "4995/17311 -  Epoch [12/10], average_loss: 3.3285\n",
      "5994/17311 -  Epoch [12/10], average_loss: 3.3372\n",
      "6993/17311 -  Epoch [12/10], average_loss: 3.2996\n",
      "7992/17311 -  Epoch [12/10], average_loss: 3.338\n",
      "8991/17311 -  Epoch [12/10], average_loss: 3.2895\n",
      "9990/17311 -  Epoch [12/10], average_loss: 3.31\n",
      "10989/17311 -  Epoch [12/10], average_loss: 3.311\n",
      "11988/17311 -  Epoch [12/10], average_loss: 3.2918\n",
      "12987/17311 -  Epoch [12/10], average_loss: 3.3517\n",
      "13986/17311 -  Epoch [12/10], average_loss: 3.2953\n",
      "14985/17311 -  Epoch [12/10], average_loss: 3.3523\n",
      "15984/17311 -  Epoch [12/10], average_loss: 3.2656\n",
      "16983/17311 -  Epoch [12/10], average_loss: 3.294\n",
      "Evaluation time: 152.3444049358368s.\n",
      "999/17311 -  Epoch [13/10], average_loss: 3.3524\n",
      "1998/17311 -  Epoch [13/10], average_loss: 3.3379\n",
      "2997/17311 -  Epoch [13/10], average_loss: 3.3387\n",
      "3996/17311 -  Epoch [13/10], average_loss: 3.3196\n",
      "4995/17311 -  Epoch [13/10], average_loss: 3.3259\n",
      "5994/17311 -  Epoch [13/10], average_loss: 3.3422\n",
      "6993/17311 -  Epoch [13/10], average_loss: 3.3022\n",
      "7992/17311 -  Epoch [13/10], average_loss: 3.3387\n",
      "8991/17311 -  Epoch [13/10], average_loss: 3.2861\n",
      "9990/17311 -  Epoch [13/10], average_loss: 3.3103\n",
      "10989/17311 -  Epoch [13/10], average_loss: 3.309\n",
      "11988/17311 -  Epoch [13/10], average_loss: 3.2889\n",
      "12987/17311 -  Epoch [13/10], average_loss: 3.3527\n",
      "13986/17311 -  Epoch [13/10], average_loss: 3.2933\n",
      "14985/17311 -  Epoch [13/10], average_loss: 3.3522\n",
      "15984/17311 -  Epoch [13/10], average_loss: 3.2656\n",
      "16983/17311 -  Epoch [13/10], average_loss: 3.2898\n",
      "Evaluation time: 154.94034790992737s.\n",
      "999/17311 -  Epoch [14/10], average_loss: 3.3521\n",
      "1998/17311 -  Epoch [14/10], average_loss: 3.3332\n",
      "2997/17311 -  Epoch [14/10], average_loss: 3.3305\n",
      "3996/17311 -  Epoch [14/10], average_loss: 3.3164\n",
      "4995/17311 -  Epoch [14/10], average_loss: 3.329\n",
      "5994/17311 -  Epoch [14/10], average_loss: 3.3369\n",
      "6993/17311 -  Epoch [14/10], average_loss: 3.3004\n",
      "7992/17311 -  Epoch [14/10], average_loss: 3.3378\n",
      "8991/17311 -  Epoch [14/10], average_loss: 3.2862\n",
      "9990/17311 -  Epoch [14/10], average_loss: 3.308\n",
      "10989/17311 -  Epoch [14/10], average_loss: 3.3066\n",
      "11988/17311 -  Epoch [14/10], average_loss: 3.2842\n",
      "12987/17311 -  Epoch [14/10], average_loss: 3.3523\n",
      "13986/17311 -  Epoch [14/10], average_loss: 3.2953\n",
      "14985/17311 -  Epoch [14/10], average_loss: 3.3497\n",
      "15984/17311 -  Epoch [14/10], average_loss: 3.2644\n",
      "16983/17311 -  Epoch [14/10], average_loss: 3.2911\n",
      "Evaluation time: 157.63660097122192s.\n",
      "999/17311 -  Epoch [15/10], average_loss: 3.351\n",
      "1998/17311 -  Epoch [15/10], average_loss: 3.3343\n",
      "2997/17311 -  Epoch [15/10], average_loss: 3.3339\n",
      "3996/17311 -  Epoch [15/10], average_loss: 3.3168\n",
      "4995/17311 -  Epoch [15/10], average_loss: 3.3249\n",
      "5994/17311 -  Epoch [15/10], average_loss: 3.3391\n",
      "6993/17311 -  Epoch [15/10], average_loss: 3.3028\n",
      "7992/17311 -  Epoch [15/10], average_loss: 3.3354\n",
      "8991/17311 -  Epoch [15/10], average_loss: 3.286\n",
      "9990/17311 -  Epoch [15/10], average_loss: 3.3063\n",
      "10989/17311 -  Epoch [15/10], average_loss: 3.3077\n",
      "11988/17311 -  Epoch [15/10], average_loss: 3.2871\n",
      "12987/17311 -  Epoch [15/10], average_loss: 3.3482\n",
      "13986/17311 -  Epoch [15/10], average_loss: 3.2944\n",
      "14985/17311 -  Epoch [15/10], average_loss: 3.3529\n",
      "15984/17311 -  Epoch [15/10], average_loss: 3.265\n",
      "16983/17311 -  Epoch [15/10], average_loss: 3.2916\n",
      "Evaluation time: 157.3167209625244s.\n",
      "999/17311 -  Epoch [16/10], average_loss: 3.3514\n",
      "1998/17311 -  Epoch [16/10], average_loss: 3.3361\n",
      "2997/17311 -  Epoch [16/10], average_loss: 3.335\n",
      "3996/17311 -  Epoch [16/10], average_loss: 3.3172\n",
      "4995/17311 -  Epoch [16/10], average_loss: 3.324\n",
      "5994/17311 -  Epoch [16/10], average_loss: 3.3341\n",
      "6993/17311 -  Epoch [16/10], average_loss: 3.2993\n",
      "7992/17311 -  Epoch [16/10], average_loss: 3.337\n",
      "8991/17311 -  Epoch [16/10], average_loss: 3.2848\n",
      "9990/17311 -  Epoch [16/10], average_loss: 3.308\n",
      "10989/17311 -  Epoch [16/10], average_loss: 3.307\n",
      "11988/17311 -  Epoch [16/10], average_loss: 3.2871\n",
      "12987/17311 -  Epoch [16/10], average_loss: 3.3516\n",
      "13986/17311 -  Epoch [16/10], average_loss: 3.2941\n",
      "14985/17311 -  Epoch [16/10], average_loss: 3.3486\n",
      "15984/17311 -  Epoch [16/10], average_loss: 3.2653\n",
      "16983/17311 -  Epoch [16/10], average_loss: 3.2929\n",
      "Evaluation time: 158.10084104537964s.\n",
      "999/17311 -  Epoch [17/10], average_loss: 3.3532\n",
      "1998/17311 -  Epoch [17/10], average_loss: 3.3351\n",
      "2997/17311 -  Epoch [17/10], average_loss: 3.334\n",
      "3996/17311 -  Epoch [17/10], average_loss: 3.3157\n",
      "4995/17311 -  Epoch [17/10], average_loss: 3.324\n",
      "5994/17311 -  Epoch [17/10], average_loss: 3.3376\n",
      "6993/17311 -  Epoch [17/10], average_loss: 3.2987\n",
      "7992/17311 -  Epoch [17/10], average_loss: 3.3383\n",
      "8991/17311 -  Epoch [17/10], average_loss: 3.2845\n",
      "9990/17311 -  Epoch [17/10], average_loss: 3.3062\n",
      "10989/17311 -  Epoch [17/10], average_loss: 3.3037\n",
      "11988/17311 -  Epoch [17/10], average_loss: 3.285\n",
      "12987/17311 -  Epoch [17/10], average_loss: 3.3475\n",
      "13986/17311 -  Epoch [17/10], average_loss: 3.2934\n",
      "14985/17311 -  Epoch [17/10], average_loss: 3.3503\n",
      "15984/17311 -  Epoch [17/10], average_loss: 3.2621\n",
      "16983/17311 -  Epoch [17/10], average_loss: 3.2914\n",
      "Evaluation time: 153.91241264343262s.\n",
      "999/17311 -  Epoch [18/10], average_loss: 3.3494\n",
      "1998/17311 -  Epoch [18/10], average_loss: 3.3352\n",
      "2997/17311 -  Epoch [18/10], average_loss: 3.3295\n",
      "3996/17311 -  Epoch [18/10], average_loss: 3.3179\n",
      "4995/17311 -  Epoch [18/10], average_loss: 3.3237\n",
      "5994/17311 -  Epoch [18/10], average_loss: 3.3375\n",
      "6993/17311 -  Epoch [18/10], average_loss: 3.3027\n",
      "7992/17311 -  Epoch [18/10], average_loss: 3.3381\n",
      "8991/17311 -  Epoch [18/10], average_loss: 3.2869\n",
      "9990/17311 -  Epoch [18/10], average_loss: 3.3097\n",
      "10989/17311 -  Epoch [18/10], average_loss: 3.3063\n",
      "11988/17311 -  Epoch [18/10], average_loss: 3.2886\n",
      "12987/17311 -  Epoch [18/10], average_loss: 3.3506\n",
      "13986/17311 -  Epoch [18/10], average_loss: 3.2927\n",
      "14985/17311 -  Epoch [18/10], average_loss: 3.3522\n",
      "15984/17311 -  Epoch [18/10], average_loss: 3.2633\n",
      "16983/17311 -  Epoch [18/10], average_loss: 3.2928\n",
      "Evaluation time: 153.72354388237s.\n",
      "999/17311 -  Epoch [19/10], average_loss: 3.3508\n",
      "1998/17311 -  Epoch [19/10], average_loss: 3.3396\n",
      "2997/17311 -  Epoch [19/10], average_loss: 3.3314\n",
      "3996/17311 -  Epoch [19/10], average_loss: 3.317\n",
      "4995/17311 -  Epoch [19/10], average_loss: 3.3259\n",
      "5994/17311 -  Epoch [19/10], average_loss: 3.3366\n",
      "6993/17311 -  Epoch [19/10], average_loss: 3.2982\n",
      "7992/17311 -  Epoch [19/10], average_loss: 3.3389\n",
      "8991/17311 -  Epoch [19/10], average_loss: 3.2854\n",
      "9990/17311 -  Epoch [19/10], average_loss: 3.3064\n",
      "10989/17311 -  Epoch [19/10], average_loss: 3.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11988/17311 -  Epoch [19/10], average_loss: 3.2826\n",
      "12987/17311 -  Epoch [19/10], average_loss: 3.3465\n",
      "13986/17311 -  Epoch [19/10], average_loss: 3.2953\n",
      "14985/17311 -  Epoch [19/10], average_loss: 3.3489\n",
      "15984/17311 -  Epoch [19/10], average_loss: 3.2646\n",
      "16983/17311 -  Epoch [19/10], average_loss: 3.293\n",
      "Evaluation time: 154.23165655136108s.\n",
      "999/17311 -  Epoch [20/10], average_loss: 3.3513\n",
      "1998/17311 -  Epoch [20/10], average_loss: 3.3349\n",
      "2997/17311 -  Epoch [20/10], average_loss: 3.3316\n",
      "3996/17311 -  Epoch [20/10], average_loss: 3.3168\n",
      "4995/17311 -  Epoch [20/10], average_loss: 3.3264\n",
      "5994/17311 -  Epoch [20/10], average_loss: 3.3369\n",
      "6993/17311 -  Epoch [20/10], average_loss: 3.2979\n",
      "7992/17311 -  Epoch [20/10], average_loss: 3.3361\n",
      "8991/17311 -  Epoch [20/10], average_loss: 3.2848\n",
      "9990/17311 -  Epoch [20/10], average_loss: 3.3071\n",
      "10989/17311 -  Epoch [20/10], average_loss: 3.3069\n",
      "11988/17311 -  Epoch [20/10], average_loss: 3.2849\n",
      "12987/17311 -  Epoch [20/10], average_loss: 3.3518\n",
      "13986/17311 -  Epoch [20/10], average_loss: 3.2948\n",
      "14985/17311 -  Epoch [20/10], average_loss: 3.349\n",
      "15984/17311 -  Epoch [20/10], average_loss: 3.2654\n",
      "16983/17311 -  Epoch [20/10], average_loss: 3.291\n",
      "Evaluation time: 155.8004777431488s.\n",
      "999/17311 -  Epoch [21/10], average_loss: 3.3513\n",
      "1998/17311 -  Epoch [21/10], average_loss: 3.3358\n",
      "2997/17311 -  Epoch [21/10], average_loss: 3.3311\n",
      "3996/17311 -  Epoch [21/10], average_loss: 3.3159\n",
      "4995/17311 -  Epoch [21/10], average_loss: 3.3247\n",
      "5994/17311 -  Epoch [21/10], average_loss: 3.3384\n",
      "6993/17311 -  Epoch [21/10], average_loss: 3.298\n",
      "7992/17311 -  Epoch [21/10], average_loss: 3.3358\n",
      "8991/17311 -  Epoch [21/10], average_loss: 3.2872\n",
      "9990/17311 -  Epoch [21/10], average_loss: 3.3084\n",
      "10989/17311 -  Epoch [21/10], average_loss: 3.3072\n",
      "11988/17311 -  Epoch [21/10], average_loss: 3.2854\n",
      "12987/17311 -  Epoch [21/10], average_loss: 3.347\n",
      "13986/17311 -  Epoch [21/10], average_loss: 3.2935\n",
      "14985/17311 -  Epoch [21/10], average_loss: 3.351\n",
      "15984/17311 -  Epoch [21/10], average_loss: 3.2637\n",
      "16983/17311 -  Epoch [21/10], average_loss: 3.2908\n",
      "Evaluation time: 155.32267785072327s.\n",
      "999/17311 -  Epoch [22/10], average_loss: 3.3495\n",
      "1998/17311 -  Epoch [22/10], average_loss: 3.3346\n",
      "2997/17311 -  Epoch [22/10], average_loss: 3.3318\n",
      "3996/17311 -  Epoch [22/10], average_loss: 3.3183\n",
      "4995/17311 -  Epoch [22/10], average_loss: 3.3228\n",
      "5994/17311 -  Epoch [22/10], average_loss: 3.3391\n",
      "6993/17311 -  Epoch [22/10], average_loss: 3.2989\n",
      "7992/17311 -  Epoch [22/10], average_loss: 3.3398\n",
      "8991/17311 -  Epoch [22/10], average_loss: 3.2858\n",
      "9990/17311 -  Epoch [22/10], average_loss: 3.3071\n",
      "10989/17311 -  Epoch [22/10], average_loss: 3.3116\n",
      "11988/17311 -  Epoch [22/10], average_loss: 3.284\n",
      "12987/17311 -  Epoch [22/10], average_loss: 3.3461\n",
      "13986/17311 -  Epoch [22/10], average_loss: 3.2953\n",
      "14985/17311 -  Epoch [22/10], average_loss: 3.3508\n",
      "15984/17311 -  Epoch [22/10], average_loss: 3.2644\n",
      "16983/17311 -  Epoch [22/10], average_loss: 3.2911\n",
      "Evaluation time: 156.867023229599s.\n",
      "999/17311 -  Epoch [23/10], average_loss: 3.3505\n",
      "1998/17311 -  Epoch [23/10], average_loss: 3.3389\n",
      "2997/17311 -  Epoch [23/10], average_loss: 3.3311\n",
      "3996/17311 -  Epoch [23/10], average_loss: 3.3194\n",
      "4995/17311 -  Epoch [23/10], average_loss: 3.3245\n",
      "5994/17311 -  Epoch [23/10], average_loss: 3.3401\n",
      "6993/17311 -  Epoch [23/10], average_loss: 3.3002\n",
      "7992/17311 -  Epoch [23/10], average_loss: 3.3351\n",
      "8991/17311 -  Epoch [23/10], average_loss: 3.2869\n",
      "9990/17311 -  Epoch [23/10], average_loss: 3.3066\n",
      "10989/17311 -  Epoch [23/10], average_loss: 3.3103\n",
      "11988/17311 -  Epoch [23/10], average_loss: 3.2866\n",
      "12987/17311 -  Epoch [23/10], average_loss: 3.3514\n",
      "13986/17311 -  Epoch [23/10], average_loss: 3.2939\n",
      "14985/17311 -  Epoch [23/10], average_loss: 3.3484\n",
      "15984/17311 -  Epoch [23/10], average_loss: 3.2664\n",
      "16983/17311 -  Epoch [23/10], average_loss: 3.2918\n",
      "Evaluation time: 156.72581791877747s.\n",
      "999/17311 -  Epoch [24/10], average_loss: 3.3514\n",
      "1998/17311 -  Epoch [24/10], average_loss: 3.3398\n",
      "2997/17311 -  Epoch [24/10], average_loss: 3.3356\n",
      "3996/17311 -  Epoch [24/10], average_loss: 3.3176\n",
      "4995/17311 -  Epoch [24/10], average_loss: 3.3253\n",
      "5994/17311 -  Epoch [24/10], average_loss: 3.3362\n",
      "6993/17311 -  Epoch [24/10], average_loss: 3.299\n",
      "7992/17311 -  Epoch [24/10], average_loss: 3.3381\n",
      "8991/17311 -  Epoch [24/10], average_loss: 3.2856\n",
      "9990/17311 -  Epoch [24/10], average_loss: 3.306\n",
      "10989/17311 -  Epoch [24/10], average_loss: 3.3126\n",
      "11988/17311 -  Epoch [24/10], average_loss: 3.288\n",
      "12987/17311 -  Epoch [24/10], average_loss: 3.3501\n",
      "13986/17311 -  Epoch [24/10], average_loss: 3.295\n",
      "14985/17311 -  Epoch [24/10], average_loss: 3.3527\n",
      "15984/17311 -  Epoch [24/10], average_loss: 3.265\n",
      "16983/17311 -  Epoch [24/10], average_loss: 3.295\n",
      "Evaluation time: 153.27826690673828s.\n",
      "999/17311 -  Epoch [25/10], average_loss: 3.3527\n",
      "1998/17311 -  Epoch [25/10], average_loss: 3.3407\n",
      "2997/17311 -  Epoch [25/10], average_loss: 3.3386\n",
      "3996/17311 -  Epoch [25/10], average_loss: 3.3213\n",
      "4995/17311 -  Epoch [25/10], average_loss: 3.3223\n",
      "5994/17311 -  Epoch [25/10], average_loss: 3.3382\n",
      "6993/17311 -  Epoch [25/10], average_loss: 3.3001\n",
      "7992/17311 -  Epoch [25/10], average_loss: 3.3402\n",
      "8991/17311 -  Epoch [25/10], average_loss: 3.2869\n",
      "9990/17311 -  Epoch [25/10], average_loss: 3.307\n",
      "10989/17311 -  Epoch [25/10], average_loss: 3.3082\n",
      "11988/17311 -  Epoch [25/10], average_loss: 3.2882\n",
      "12987/17311 -  Epoch [25/10], average_loss: 3.3482\n",
      "13986/17311 -  Epoch [25/10], average_loss: 3.2943\n",
      "14985/17311 -  Epoch [25/10], average_loss: 3.349\n",
      "15984/17311 -  Epoch [25/10], average_loss: 3.2649\n",
      "16983/17311 -  Epoch [25/10], average_loss: 3.2925\n",
      "Evaluation time: 154.2301003932953s.\n",
      "999/17311 -  Epoch [26/10], average_loss: 3.3493\n",
      "1998/17311 -  Epoch [26/10], average_loss: 3.3337\n",
      "2997/17311 -  Epoch [26/10], average_loss: 3.3319\n",
      "3996/17311 -  Epoch [26/10], average_loss: 3.3189\n",
      "4995/17311 -  Epoch [26/10], average_loss: 3.3261\n",
      "5994/17311 -  Epoch [26/10], average_loss: 3.3384\n",
      "6993/17311 -  Epoch [26/10], average_loss: 3.2977\n",
      "7992/17311 -  Epoch [26/10], average_loss: 3.3356\n",
      "8991/17311 -  Epoch [26/10], average_loss: 3.2885\n",
      "9990/17311 -  Epoch [26/10], average_loss: 3.3091\n",
      "10989/17311 -  Epoch [26/10], average_loss: 3.3071\n",
      "11988/17311 -  Epoch [26/10], average_loss: 3.2935\n",
      "12987/17311 -  Epoch [26/10], average_loss: 3.3527\n",
      "13986/17311 -  Epoch [26/10], average_loss: 3.2964\n",
      "14985/17311 -  Epoch [26/10], average_loss: 3.3499\n",
      "15984/17311 -  Epoch [26/10], average_loss: 3.2667\n",
      "16983/17311 -  Epoch [26/10], average_loss: 3.2926\n",
      "Evaluation time: 154.79959559440613s.\n",
      "999/17311 -  Epoch [27/10], average_loss: 3.35\n",
      "1998/17311 -  Epoch [27/10], average_loss: 3.3377\n",
      "2997/17311 -  Epoch [27/10], average_loss: 3.3338\n",
      "3996/17311 -  Epoch [27/10], average_loss: 3.3183\n",
      "4995/17311 -  Epoch [27/10], average_loss: 3.3265\n",
      "5994/17311 -  Epoch [27/10], average_loss: 3.3378\n",
      "6993/17311 -  Epoch [27/10], average_loss: 3.3012\n",
      "7992/17311 -  Epoch [27/10], average_loss: 3.3394\n",
      "8991/17311 -  Epoch [27/10], average_loss: 3.2892\n",
      "9990/17311 -  Epoch [27/10], average_loss: 3.3088\n",
      "10989/17311 -  Epoch [27/10], average_loss: 3.3072\n",
      "11988/17311 -  Epoch [27/10], average_loss: 3.2901\n",
      "12987/17311 -  Epoch [27/10], average_loss: 3.3544\n",
      "13986/17311 -  Epoch [27/10], average_loss: 3.2946\n",
      "14985/17311 -  Epoch [27/10], average_loss: 3.3502\n",
      "15984/17311 -  Epoch [27/10], average_loss: 3.2657\n",
      "16983/17311 -  Epoch [27/10], average_loss: 3.2929\n",
      "Evaluation time: 153.80021405220032s.\n",
      "999/17311 -  Epoch [28/10], average_loss: 3.3508\n",
      "1998/17311 -  Epoch [28/10], average_loss: 3.3364\n",
      "2997/17311 -  Epoch [28/10], average_loss: 3.3322\n",
      "3996/17311 -  Epoch [28/10], average_loss: 3.3181\n",
      "4995/17311 -  Epoch [28/10], average_loss: 3.3268\n",
      "5994/17311 -  Epoch [28/10], average_loss: 3.3378\n",
      "6993/17311 -  Epoch [28/10], average_loss: 3.2978\n",
      "7992/17311 -  Epoch [28/10], average_loss: 3.3386\n",
      "8991/17311 -  Epoch [28/10], average_loss: 3.2881\n",
      "9990/17311 -  Epoch [28/10], average_loss: 3.3075\n",
      "10989/17311 -  Epoch [28/10], average_loss: 3.3094\n",
      "11988/17311 -  Epoch [28/10], average_loss: 3.294\n",
      "12987/17311 -  Epoch [28/10], average_loss: 3.3503\n",
      "13986/17311 -  Epoch [28/10], average_loss: 3.2938\n",
      "14985/17311 -  Epoch [28/10], average_loss: 3.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15984/17311 -  Epoch [28/10], average_loss: 3.2665\n",
      "16983/17311 -  Epoch [28/10], average_loss: 3.2926\n",
      "Evaluation time: 154.22381401062012s.\n",
      "999/17311 -  Epoch [29/10], average_loss: 3.3507\n",
      "1998/17311 -  Epoch [29/10], average_loss: 3.3349\n",
      "2997/17311 -  Epoch [29/10], average_loss: 3.3326\n",
      "3996/17311 -  Epoch [29/10], average_loss: 3.3203\n",
      "4995/17311 -  Epoch [29/10], average_loss: 3.3249\n",
      "5994/17311 -  Epoch [29/10], average_loss: 3.3375\n",
      "6993/17311 -  Epoch [29/10], average_loss: 3.2977\n",
      "7992/17311 -  Epoch [29/10], average_loss: 3.3365\n",
      "8991/17311 -  Epoch [29/10], average_loss: 3.2871\n",
      "9990/17311 -  Epoch [29/10], average_loss: 3.3042\n",
      "10989/17311 -  Epoch [29/10], average_loss: 3.3075\n",
      "11988/17311 -  Epoch [29/10], average_loss: 3.2901\n",
      "12987/17311 -  Epoch [29/10], average_loss: 3.3493\n",
      "13986/17311 -  Epoch [29/10], average_loss: 3.2969\n",
      "14985/17311 -  Epoch [29/10], average_loss: 3.3498\n",
      "15984/17311 -  Epoch [29/10], average_loss: 3.2667\n",
      "16983/17311 -  Epoch [29/10], average_loss: 3.2963\n",
      "Evaluation time: 153.11065340042114s.\n",
      "999/17311 -  Epoch [30/10], average_loss: 3.3508\n",
      "1998/17311 -  Epoch [30/10], average_loss: 3.3355\n",
      "2997/17311 -  Epoch [30/10], average_loss: 3.3334\n",
      "3996/17311 -  Epoch [30/10], average_loss: 3.3225\n",
      "4995/17311 -  Epoch [30/10], average_loss: 3.3279\n",
      "5994/17311 -  Epoch [30/10], average_loss: 3.3377\n",
      "6993/17311 -  Epoch [30/10], average_loss: 3.2968\n",
      "7992/17311 -  Epoch [30/10], average_loss: 3.339\n",
      "8991/17311 -  Epoch [30/10], average_loss: 3.2944\n",
      "9990/17311 -  Epoch [30/10], average_loss: 3.3066\n",
      "10989/17311 -  Epoch [30/10], average_loss: 3.309\n",
      "11988/17311 -  Epoch [30/10], average_loss: 3.2928\n",
      "12987/17311 -  Epoch [30/10], average_loss: 3.3524\n",
      "13986/17311 -  Epoch [30/10], average_loss: 3.2931\n",
      "14985/17311 -  Epoch [30/10], average_loss: 3.3511\n",
      "15984/17311 -  Epoch [30/10], average_loss: 3.2648\n",
      "16983/17311 -  Epoch [30/10], average_loss: 3.2927\n",
      "Evaluation time: 155.84174728393555s.\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "#model = BiRNN(75, HIDDEN_SIZE, LINEAR_INPUT_SIZE, len(train_dataset.classes)).to(device)\n",
    "model = NewBiRNN(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_INPUT_SIZE, len(train_dataset.classes)).to(device)\n",
    "\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    print(\"Pretrained model not found\")\n",
    "\n",
    "PRINT_STEP = 999\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=L2_WEIGTH_DECAY\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):\n",
    "    s_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (sequence, labels) in enumerate(train_loader, 1):\n",
    "        #print(sequence.shape)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        sequence = sequence.reshape(-1, 1, 25 * 3).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "        \n",
    "        #print(sequence.shape)\n",
    "        #print(labels.shape)\n",
    "        #break\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(sequence)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % PRINT_STEP == 0:\n",
    "            print(f\"{i}/{len(train_loader)} -  Epoch [{epoch + 1}/{EPOCHS}], average_loss: {round(total_loss / PRINT_STEP, 4)}\")\n",
    "            total_loss = 0.0\n",
    "    \n",
    "    print(f\"Evaluation time: {time.time() - s_time}s.\")\n",
    "    #break\n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: [0/2509]\n",
      "Processed: [49/2509]\n",
      "Processed: [98/2509]\n",
      "Processed: [147/2509]\n",
      "Processed: [196/2509]\n",
      "Processed: [245/2509]\n",
      "Processed: [294/2509]\n",
      "Processed: [343/2509]\n",
      "Processed: [392/2509]\n",
      "Processed: [441/2509]\n",
      "Processed: [490/2509]\n",
      "Processed: [539/2509]\n",
      "Processed: [588/2509]\n",
      "Processed: [637/2509]\n",
      "Processed: [686/2509]\n",
      "Processed: [735/2509]\n",
      "Processed: [784/2509]\n",
      "Processed: [833/2509]\n",
      "Processed: [882/2509]\n",
      "Processed: [931/2509]\n",
      "Processed: [980/2509]\n",
      "Processed: [1029/2509]\n",
      "Processed: [1078/2509]\n",
      "Processed: [1127/2509]\n",
      "Processed: [1176/2509]\n",
      "Processed: [1225/2509]\n",
      "Processed: [1274/2509]\n",
      "Processed: [1323/2509]\n",
      "Processed: [1372/2509]\n",
      "Processed: [1421/2509]\n",
      "Processed: [1470/2509]\n",
      "Processed: [1519/2509]\n",
      "Processed: [1568/2509]\n",
      "Processed: [1617/2509]\n",
      "Processed: [1666/2509]\n",
      "Processed: [1715/2509]\n",
      "Processed: [1764/2509]\n",
      "Processed: [1813/2509]\n",
      "Processed: [1862/2509]\n",
      "Processed: [1911/2509]\n",
      "Processed: [1960/2509]\n",
      "Processed: [2009/2509]\n",
      "Processed: [2058/2509]\n",
      "Processed: [2107/2509]\n",
      "Processed: [2156/2509]\n",
      "Processed: [2205/2509]\n",
      "Processed: [2254/2509]\n",
      "Processed: [2303/2509]\n",
      "Processed: [2352/2509]\n",
      "Processed: [2401/2509]\n",
      "Processed: [2450/2509]\n",
      "Processed: [2499/2509]\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "with torch.no_grad():\n",
    "    steps = 10\n",
    "    def_dict = {\n",
    "        \"correct\": 0,\n",
    "        \"above\": 0,\n",
    "    }\n",
    "    thresholds = [(round((1 / steps) * (i + 1), 4), def_dict.copy()) for i in range(steps)]\n",
    "    total = 0\n",
    "\n",
    "    for i, (sequence, labels) in enumerate(test_loader):\n",
    "        sequence = sequence.reshape(-1, 1, 25 * 3).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "        \n",
    "        outputs = model(sequence)\n",
    "\n",
    "        for val_th, res_dict in thresholds:\n",
    "            res_dict[\"above\"] += (outputs.data > val_th).sum().item()\n",
    "            \n",
    "            # Correctly predicted\n",
    "            for record_i, label_i in zip(*torch.where(outputs.data > val_th)):\n",
    "                if label_i == labels[record_i]:\n",
    "                    res_dict[\"correct\"] += 1\n",
    "        \n",
    "        total += labels.size(0)\n",
    "\n",
    "        if i % 49 == 0:\n",
    "            print(f\"Processed: [{i}/{len(test_dataset)}]\")\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------0.1---------------\n",
      "Test Precision: 4.8892%\n",
      "Test Recall: 62.1731%\n",
      "Test f1_score: 0.0907\n",
      "\n",
      "---------------0.2---------------\n",
      "Test Precision: 5.3056%\n",
      "Test Recall: 58.6124%\n",
      "Test f1_score: 0.0973\n",
      "\n",
      "---------------0.3---------------\n",
      "Test Precision: 5.5928%\n",
      "Test Recall: 56.302%\n",
      "Test f1_score: 0.1017\n",
      "\n",
      "---------------0.4---------------\n",
      "Test Precision: 5.8304%\n",
      "Test Recall: 54.4377%\n",
      "Test f1_score: 0.1053\n",
      "\n",
      "---------------0.5---------------\n",
      "Test Precision: 6.0519%\n",
      "Test Recall: 52.7792%\n",
      "Test f1_score: 0.1086\n",
      "\n",
      "---------------0.6---------------\n",
      "Test Precision: 6.2751%\n",
      "Test Recall: 51.1258%\n",
      "Test f1_score: 0.1118\n",
      "\n",
      "---------------0.7---------------\n",
      "Test Precision: 6.5132%\n",
      "Test Recall: 49.2905%\n",
      "Test f1_score: 0.1151\n",
      "\n",
      "---------------0.8---------------\n",
      "Test Precision: 6.8073%\n",
      "Test Recall: 47.025%\n",
      "Test f1_score: 0.1189\n",
      "\n",
      "---------------0.9---------------\n",
      "Test Precision: 7.2318%\n",
      "Test Recall: 43.3316%\n",
      "Test f1_score: 0.1239\n",
      "\n",
      "---------------1.0---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "--------------------------------\n",
      "-------------- AP --------------\n",
      "AP: 28.4198%\n"
     ]
    }
   ],
   "source": [
    "# Calculation of evalution metrics for every threshold\n",
    "ap_score = 0\n",
    "for th, values in thresholds:\n",
    "    old_recall = 0\n",
    "\n",
    "    recall = values[\"correct\"] / total\n",
    "\n",
    "    if values[\"above\"] > 0:\n",
    "        precision = values[\"correct\"] / values[\"above\"]\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        f1_score = 0\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 15 + f\"{th}\" + \"-\" * 15)\n",
    "    print(f\"Test Precision: {round(100 * precision, 4)}%\")\n",
    "    print(f\"Test Recall: {round(100 * recall, 4)}%\")\n",
    "    print(f\"Test f1_score: {round(f1_score, 4)}\")\n",
    "\n",
    "    ap_score += (recall - old_recall) * precision\n",
    "    old_recall = recall\n",
    "\n",
    "# AP - score\n",
    "print(\"-\" * 32)\n",
    "print(\"-\" * 14 + \" AP \" + \"-\" * 14)\n",
    "print(f\"AP: {round(ap_score * 100, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#  - evaluacne metriky\n",
    "#  - format dat pre trenovanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Evalution metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "#  - Precision: the ratio of correctly annotated frames and all the model-annotated frames on test sequences\n",
    "#  - spravne anotovane (correctly annotates)\n",
    "#  - zoberiem threshold pre kazdy output v kazdom snimku --> pocet tried do ktorych sa klasifikoval snimok (all model-annotated frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
