{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "INPUT_SIZE = 25 * 3\n",
    "HIDDEN_SIZE = 1024 // 2\n",
    "EMBEDDING_INPUT_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "L2_WEIGTH_DECAY = 0.0001\n",
    "EPOCHS = 200\n",
    "\n",
    "LABELS = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4,\n",
    "    6: 5,\n",
    "    7: 6,\n",
    "    8: 7,\n",
    "    9: 8,\n",
    "    10: 9,\n",
    "    11: 10,\n",
    "    13: 11,\n",
    "    15: 12,\n",
    "    17: 13,\n",
    "    19: 14,\n",
    "    20: 15,\n",
    "    22: 16,\n",
    "    23: 17,\n",
    "    25: 18,\n",
    "    28: 19,\n",
    "    29: 20,\n",
    "    30: 21,\n",
    "    31: 22,\n",
    "    32: 23,\n",
    "    33: 24,\n",
    "    34: 25,\n",
    "    35: 26,\n",
    "    36: 27,\n",
    "    37: 28,\n",
    "    38: 29,\n",
    "    39: 30,\n",
    "    40: 31,\n",
    "    41: 32,\n",
    "    42: 33,\n",
    "    43: 34,\n",
    "    44: 35,\n",
    "    45: 36,\n",
    "    46: 37,\n",
    "    47: 38,\n",
    "    48: 39,\n",
    "    49: 40,\n",
    "    50: 41,\n",
    "    51: 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMovementDataset(IterableDataset):\n",
    "    \n",
    "    NUMBER_OF_JOINTS = 25\n",
    "    NUMBER_OF_AXES = 3\n",
    "    \n",
    "    def __init__(self, root: str, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.data_files = list(sorted(os.listdir(self.root)))\n",
    "        self.file_frames: List[int] = []\n",
    "        \n",
    "        self.classes = LABELS\n",
    "        \n",
    "        self.loaded_data = dict()\n",
    "        for file_name in self.data_files:\n",
    "            with open(os.path.join(self.root, file_name), \"r\") as f:\n",
    "                self.loaded_data[file_name] = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    def _get_file_length(self, file_data: List[str]):\n",
    "        header = file_data[0].split()[-1].split(\"_\")\n",
    "        return int(header[-1])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, file_name in enumerate(self.data_files):\n",
    "            #action_file = os.path.join(self.root, file_name)\n",
    "            #with open(action_file, \"r\") as f:\n",
    "            #    data_str = f.read().rstrip('\\n').split('\\n')\n",
    "            data_str = self.loaded_data[file_name]\n",
    "            \n",
    "            sequence_length = self._get_file_length(data_str)\n",
    "            \n",
    "            all_frames = []\n",
    "            for frame in data_str[2:]:  # first two header lines in the file\n",
    "                all_frames.append(\n",
    "                    [triple.split(\", \") for triple in frame.split(\"; \")]\n",
    "                )\n",
    "            \n",
    "            all_frames = np.array(all_frames, dtype=np.float32)\n",
    "            '''\n",
    "            frame = np.array(\n",
    "                [\n",
    "                    triple.split(\", \") for triple in data_str[line_indx].split(\";\")\n",
    "                ],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            '''\n",
    "            assert all_frames.shape == (sequence_length, self.NUMBER_OF_JOINTS, self.NUMBER_OF_AXES)\n",
    "    \n",
    "            # get sequence label\n",
    "            #target = [self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]] * sequence_length\n",
    "            #target = np.array(target)\n",
    "            label = self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]\n",
    "            target = np.zeros(len(self.classes), dtype=np.float64)\n",
    "            target[label] = 1.0\n",
    "\n",
    "            #target = torch.from_numpy(target).double()\n",
    "        \n",
    "            if self.transforms:\n",
    "                all_frames = self.transforms(all_frames)\n",
    "\n",
    "            yield all_frames, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, linear_input_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(self.hidden_size * 2, linear_input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(linear_input_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward to LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # output format: (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.embedding(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.do(out)\n",
    "        out = self.classifier(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewBiRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, lstm_hidden_size: int, embedding_output_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = lstm_hidden_size\n",
    "        self.num_layers = 2  # bi-LSTM\n",
    "        \n",
    "        # Embedding part, from 75 -> 64 size\n",
    "        self.embedding = nn.Linear(input_size, embedding_output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_output_size, lstm_hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.hidden_size * self.num_layers, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Embedding\n",
    "        out = self.embedding(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        #print(f\"O: {out.size()}\")  # (143, 1, 64) (batch, seq, features)\n",
    "        #out = out.reshape(1, 1, -1)\n",
    "        \n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "        \n",
    "        out = self.do(out[:, -1, :])\n",
    "        out = self.classifier(out)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch dataloader\n",
    "class MovementsDataset(Dataset):\n",
    "    \n",
    "    NUMBER_OF_JOINTS = 25\n",
    "    NUMBER_OF_AXES = 3\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.data_files = list(sorted(os.listdir(self.root)))\n",
    "        self.file_frames: List[int] = []\n",
    "\n",
    "        self.classes = LABELS\n",
    "        \n",
    "        # Load number of frames for every file\n",
    "        for fn in self.data_files:\n",
    "            with open(os.path.join(self.root, fn)) as f:\n",
    "                header = f.readline().split()[-1].split(\"_\")\n",
    "                \n",
    "                self.file_frames.append(int(header[-1]))  # last element - number_of_frames\n",
    "        \n",
    "    def _get_file_index(self, frame_indx) -> Tuple[int, int]:\n",
    "        start_indx = frame_indx\n",
    "        for i, nof in enumerate(self.file_frames):\n",
    "            if start_indx < nof:\n",
    "                # print(f\"{start_indx} - {i}\")\n",
    "                return i, start_indx\n",
    "            else:\n",
    "                start_indx -= nof\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        file_indx, line_indx = self._get_file_index(indx)\n",
    "        action_file = os.path.join(self.root, self.data_files[file_indx])\n",
    "        \n",
    "        with open(action_file, \"r\") as f:\n",
    "            data_str = f.read().rstrip('\\n').split('\\n')\n",
    "        \n",
    "        line_indx += 2  # first two header lines in the file   \n",
    "        frame = np.array([triple.split(\", \") for triple in data_str[line_indx].split(\";\")], dtype=np.float32)\n",
    "        assert frame.shape == (self.NUMBER_OF_JOINTS, self.NUMBER_OF_AXES)\n",
    "        \n",
    "        target = self.classes[int(data_str[0].split()[-1].split(\"_\")[1])]\n",
    "        \n",
    "        if self.transforms:\n",
    "            frame = self.transforms(frame)\n",
    "        \n",
    "        return frame, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IterableMovementDataset(\n",
    "    \"../data/cross-view/train\",\n",
    "    transforms=transforms.ToTensor()\n",
    ")\n",
    "test_dataset = IterableMovementDataset(\n",
    "    \"../data/cross-view/val\",\n",
    "    transforms=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset) #, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset) #, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "#  - Adam optimizer *\n",
    "#  - LR = 0.0005 *\n",
    "#  - batch = 1 *\n",
    "#  - L2 weight decay = 0.0001 *\n",
    "#  - dropout = 0.5 *\n",
    "#  - 200 epochs\n",
    "#  - Embedding - 64 *\n",
    "#  - Hidden-state - 1024 --> halved for Bi-LSTM *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name: str, epoch: int):\n",
    "    torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "    with open(\"last_checkpoint\", \"w\") as lf:\n",
    "        lf.write(str(epoch))\n",
    "    print(f\"Model saved into: {model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741\n"
     ]
    }
   ],
   "source": [
    "max_size = 0\n",
    "\n",
    "for i, (sequence, labels) in enumerate(train_loader, 1):\n",
    "    bs = sequence.size(2)\n",
    "    if bs > max_size:\n",
    "        max_size = bs\n",
    "    #print(bs)\n",
    "    \n",
    "print(max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 24, 25])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10725])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 55575])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros(1, 741 * 3 * 25)\n",
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:, :10725] = sequence.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1190, -0.1671,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:, 10723:10727]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-eb7b9d9da4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robo-dip/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robo-dip/venv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 932\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robo-dip/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robo-dip/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:18"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "#model = BiRNN(75, HIDDEN_SIZE, LINEAR_INPUT_SIZE, len(train_dataset.classes)).to(device)\n",
    "model = NewBiRNN(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_INPUT_SIZE, len(train_dataset.classes)).to(device)\n",
    "\n",
    "#INPUT_SIZE = 741 * 3 * 25\n",
    "#model = NewBiRNN(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_INPUT_SIZE, len(train_dataset.classes)).to(device)\n",
    "\n",
    "start_epoch = 0\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "    with open(\"last_checkpoint\", \"r\") as lf:\n",
    "        start_epoch = int(lf.read())\n",
    "    print(f\"Model loaded with epoch: {start_epoch}\")\n",
    "else:\n",
    "    print(\"Pretrained model not found\")\n",
    "\n",
    "SAVE_CHECHPOINT = 50\n",
    "PRINT_STEP = 999\n",
    "board_writer = SummaryWriter()\n",
    "\n",
    "criterion = nn.BCELoss()  #nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=L2_WEIGTH_DECAY\n",
    ")\n",
    "\n",
    "#max_action_frames = 741\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
    "    s_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_iterations_per_epoch = len(train_loader)\n",
    "    \n",
    "    for i, (sequence, labels) in enumerate(train_loader, 1):\n",
    "        #print(sequence.shape)\n",
    "        #print(labels.shape)\n",
    "        #print(labels)\n",
    "        \n",
    "        sequence = sequence.reshape(-1, 1, 25 * 3).to(device)\n",
    "        \n",
    "        #target = torch.zeros(1, 741 * 3 * 25).to(device)\n",
    "        #sequence = sequence.reshape(1, -1).to(device)\n",
    "        #sequence_size = sequence.size(1)\n",
    "        #target[:, :sequence_size] = sequence\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #print(sequence.shape)\n",
    "        #print(target.shape)\n",
    "        #print(labels)\n",
    "        #break\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(sequence).double()\n",
    "        #outputs = model(target).double()\n",
    "        \n",
    "        labels = labels.repeat(outputs.size(0), 1)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        #break\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #print(total_loss)\n",
    "        #break\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % PRINT_STEP == 0:\n",
    "            average_loss = total_loss / PRINT_STEP\n",
    "            print(f\"{i}/{len(train_loader)} -  Epoch [{epoch + 1}/{EPOCHS}], average_loss: {round(average_loss, 6)}\")\n",
    "            total_loss = 0.0\n",
    "\n",
    "            board_writer.add_scalar(\n",
    "                'Average_Loss/train',\n",
    "                average_loss,\n",
    "                (epoch * total_iterations_per_epoch) + i\n",
    "            )\n",
    "\n",
    "    print(f\"Evaluation time: {time.time() - s_time}s.\")\n",
    "\n",
    "    if (epoch + 1) % SAVE_CHECHPOINT == 0:\n",
    "        save_model(model, f\"model_{epoch}\", epoch)\n",
    "\n",
    "save_model(model, \"model\", epoch)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model(model, \"model\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(dict_results, total_records):\n",
    "    with open(\"model_results.json\", \"w\") as jf:\n",
    "        json.dump({\n",
    "            \"thresholds\": dict_results,\n",
    "            \"total_records\": total_records\n",
    "        }, jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: [0/837]\n",
      "Processed: [49/837]\n",
      "Processed: [98/837]\n",
      "Processed: [147/837]\n",
      "Processed: [196/837]\n",
      "Processed: [245/837]\n",
      "Processed: [294/837]\n",
      "Processed: [343/837]\n",
      "Processed: [392/837]\n",
      "Processed: [441/837]\n",
      "Processed: [490/837]\n",
      "Processed: [539/837]\n",
      "Processed: [588/837]\n",
      "Processed: [637/837]\n",
      "Processed: [686/837]\n",
      "Processed: [735/837]\n",
      "Processed: [784/837]\n",
      "Processed: [833/837]\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "with torch.no_grad():\n",
    "    steps = 10\n",
    "    def_dict = {\n",
    "        \"correct\": 0,\n",
    "        \"above\": 0,\n",
    "    }\n",
    "    thresholds = [(round((1 / steps) * (i + 1), 4), def_dict.copy()) for i in range(steps)]\n",
    "    total = 0\n",
    "\n",
    "    for i, (sequence, labels) in enumerate(test_loader):\n",
    "        #break\n",
    "        sequence = sequence.reshape(-1, 1, 25 * 3).to(device)\n",
    "        labels = torch.argmax(labels).repeat(outputs.size(0)).to(device)\n",
    "        \n",
    "        #print(labels.size())\n",
    "        #print(labels)\n",
    "        #break\n",
    "        \n",
    "        #labels = labels.repeat(outputs.size(0), 1).to(device)\n",
    "        #labels = labels.reshape(-1).to(device)\n",
    "        outputs = model(sequence)\n",
    "\n",
    "        for val_th, res_dict in thresholds:\n",
    "            res_dict[\"above\"] += (outputs.data > val_th).sum().item()\n",
    "            \n",
    "            # Correctly predicted\n",
    "            for record_i, label_i in zip(*torch.where(outputs.data > val_th)):\n",
    "                if label_i == labels[record_i]:\n",
    "                    res_dict[\"correct\"] += 1\n",
    "        \n",
    "        total += labels.size(0)\n",
    "\n",
    "        if i % 49 == 0:\n",
    "            print(f\"Processed: [{i}/{len(test_dataset)}]\")\n",
    "\n",
    "# Save dictionary results\n",
    "save_results(thresholds, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs.data > 0.06).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(thresholds, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(\"model_cv/model_results.json\", \"r\") as jf:\n",
    "    data = json.load(jf)\n",
    "\n",
    "thresholds = data[\"thresholds\"]\n",
    "total = data[\"total_records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------0.1---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.2---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.3---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.4---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.5---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.6---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.7---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.8---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "\n",
      "---------------0.9---------------\n",
      "Test Precision: 0%\n",
      "Test Recall: 0.0%\n",
      "Test f1_score: 0\n",
      "--------------------------------\n",
      "-------------- AP --------------\n",
      "AP: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Calculation of evalution metrics for every threshold\n",
    "ap_score = 0\n",
    "for th, values in thresholds[:-1]:\n",
    "    old_recall = 0\n",
    "\n",
    "    recall = values[\"correct\"] / total\n",
    "\n",
    "    if values[\"above\"] > 0:\n",
    "        precision = values[\"correct\"] / values[\"above\"]\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        f1_score = 0\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 15 + f\"{th}\" + \"-\" * 15)\n",
    "    print(f\"Test Precision: {round(100 * precision, 4)}%\")\n",
    "    print(f\"Test Recall: {round(100 * recall, 4)}%\")\n",
    "    print(f\"Test f1_score: {round(f1_score, 4)}\")\n",
    "\n",
    "    ap_score += (recall - old_recall) * precision\n",
    "    old_recall = recall\n",
    "\n",
    "# AP - score\n",
    "print(\"-\" * 32)\n",
    "print(\"-\" * 14 + \" AP \" + \"-\" * 14)\n",
    "print(f\"AP: {round(ap_score * 100, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#  - evaluacne metriky\n",
    "#  - format dat pre trenovanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Evalution metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "#  - Precision: the ratio of correctly annotated frames and all the model-annotated frames on test sequences\n",
    "#  - spravne anotovane (correctly annotates)\n",
    "#  - zoberiem threshold pre kazdy output v kazdom snimku --> pocet tried do ktorych sa klasifikoval snimok (all model-annotated frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
